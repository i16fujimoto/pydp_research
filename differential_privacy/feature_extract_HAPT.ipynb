{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extract HAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy \n",
    "import numpy as np\n",
    "\n",
    "# Importing Scipy \n",
    "import scipy as sp\n",
    "\n",
    "# Importing Pandas Library \n",
    "import pandas as pd\n",
    "\n",
    "# import glob function to scrap files path\n",
    "from glob import glob\n",
    "\n",
    "# import display() for better visualitions of DataFrames and arrays\n",
    "from IPython.display import display\n",
    "\n",
    "# import pyplot for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('bmh') # for better plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. RawData folder Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_data_paths = sorted(glob(\"HAPT Data set/RawData/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting acc file paths only\n",
    "Raw_acc_paths=Raw_data_paths[0:61]\n",
    "\n",
    "# Selecting gyro file paths only\n",
    "Raw_gyro_paths=Raw_data_paths[61:122]\n",
    "\n",
    "# printing info related to acc and gyro files\n",
    "print ((\"RawData folder contains in total {:d} file \").format(len(Raw_data_paths)))\n",
    "print ((\"The first {:d} are Acceleration files:\").format(len(Raw_acc_paths)))\n",
    "print ((\"The second {:d} are Gyroscope files:\").format(len(Raw_gyro_paths)))\n",
    "print (\"The last file is a labels file\")\n",
    "\n",
    "# printing 'labels.txt' path\n",
    "print (\"labels file path is:\",Raw_data_paths[122])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Define import acc and gyro files function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    FUNCTION: import_raw_signals(path,columns)\n",
    "#    #           1- Import acc or gyro file                            #\n",
    "#    #           2- convert from txt format to float format            #\n",
    "#    #           3- convert to a dataframe & insert column names       #\n",
    "\n",
    "\n",
    "def import_raw_signals(file_path, columns):\n",
    "    # Inputs:                                                                            #\n",
    "    #   file_path: A string contains the path of the \"acc\" or \"gyro\" txt file            #\n",
    "    #   columns: A list of strings contains the column names in order.                   #\n",
    "    # Outputs:                                                                           #\n",
    "    #   dataframe: A pandas Dataframe contains \"acc\" or \"gyro\" data in a float format    #\n",
    "    #             with columns names.  \n",
    "    \n",
    "    # open the txt file\n",
    "    opened_file =open(file_path,'r')\n",
    "\n",
    "    # Create a list\n",
    "    opened_file_list=[]\n",
    "    \n",
    "    # loop over each line in the opened_file\n",
    "    # convert each element from \"txt format\" to ”float” \n",
    "    # store each raw in a list\n",
    "    for line in opened_file:\n",
    "        opened_file_list.append([float(element) for element in line.split()])\n",
    "\n",
    "    # convert the list of lists（List of 2 dimensions） into 2D numpy array(computationally efficient)\n",
    "    data=np.array(opened_file_list)\n",
    "\n",
    "\n",
    "    # Create a pandas dataframe from this 2D numpy array with column names\n",
    "    data_frame=pd.DataFrame(data=data,columns=columns)\n",
    "    # display(data_frame)\n",
    "\n",
    "    # return the data frame\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Importing Files and Storing DataFrames in *raw_dic*\n",
    "\n",
    "- Combine each two data frames (acc dataframe and gyro data frame) in One Df and Store it in a dictionnary called **raw_dic** with the key: expXX_userYY \n",
    "\n",
    "- accデータフレームとgyroデータフレームを1つのDfにまとめ、expXX_userYYというキーで**raw_dic**という辞書に格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty dictionary where all dataframes will be stored\n",
    "raw_dic={}\n",
    "\n",
    "# creating list contains columns names of an acc file\n",
    "raw_acc_columns=['acc_X','acc_Y','acc_Z']\n",
    "\n",
    "# creating list contains gyro files columns names\n",
    "raw_gyro_columns=['gyro_X','gyro_Y','gyro_Z']\n",
    "\n",
    "# loop for to convert  each \"acc file\" into data frame of floats and store it in a dictionnary.\n",
    "for path_index in range(0,61):\n",
    "        \n",
    "        # extracting the file name only and use it as key:[expXX_userXX] without \"acc\" or \"gyro\"\n",
    "        key= Raw_data_paths[path_index][-16:-4]\n",
    "        # print(key) ex.) exp01_user01\n",
    "        \n",
    "        # Applying the function defined above to one acc_file and store the output in a DataFrame\n",
    "        raw_acc_data_frame=import_raw_signals(Raw_data_paths[path_index],raw_acc_columns)\n",
    "        \n",
    "        # By shifting the path_index by 61 we find the index of the gyro file related to same experiment_ID\n",
    "        # Applying the function defined above to one gyro_file and store the output in a DataFrame\n",
    "        raw_gyro_data_frame=import_raw_signals(Raw_data_paths[path_index+61],raw_gyro_columns)\n",
    "        \n",
    "        # concatenate acc_df and gyro_df in one DataFrame\n",
    "        raw_signals_data_frame=pd.concat([raw_acc_data_frame, raw_gyro_data_frame], axis=1)\n",
    "        \n",
    "        # Store this new DataFrame in a raw_dic , with the key extracted above\n",
    "        raw_dic[key]=raw_signals_data_frame\n",
    "\n",
    "print(raw_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dic is a dictionary contains 61 combined DF (acc_df and gyro_df)\n",
    "print('raw_dic contains %d DataFrame' % len(raw_dic))\n",
    "\n",
    "# print the first 3 rows of dataframe exp01_user01\n",
    "display(raw_dic['exp01_user01'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step14'></a>\n",
    "### I.4.  Define Import_labels_file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    FUNCTION: import_raw_labels_file(path, columns)\n",
    "#    #      1- Import labels.txt                                           #\n",
    "#    #      2- convert data from txt format to int                         #\n",
    "#    #      3- convert integer data to a dataframe & insert columns names  #\n",
    "\n",
    "def import_labels_file(path,columns):\n",
    "    \n",
    "    # Inputs:                                                                            #\n",
    "    #   path: A string contains the path of \"labels.txt\"                                 #\n",
    "    #   columns: A list of strings contains the columns names in order.                  #\n",
    "    # Outputs:                                                                           #\n",
    "    #   dataframe: A pandas Dataframe contains labels  data in int format                #\n",
    "    #             with columns names.                                                    #\n",
    "    \n",
    "    # open the txt file\n",
    "    labels_file =open(path,'r')\n",
    "    \n",
    "    # creating a list \n",
    "    labels_file_list=[]\n",
    "    \n",
    "    \n",
    "    #Store each row in a list ,convert its list elements to int type\n",
    "    for line in labels_file:\n",
    "        labels_file_list.append([int(element) for element in line.split()])\n",
    "    # convert the list of lists into 2D numpy array \n",
    "    data=np.array(labels_file_list)\n",
    "    \n",
    "    # Create a pandas dataframe from this 2D numpy array with column names \n",
    "    data_frame=pd.DataFrame(data=data,columns=columns)\n",
    "    \n",
    "    # returning the labels dataframe \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step15'></a>\n",
    "### I.5. Apply import_labels_file\n",
    "\n",
    "- Apply import_raw_labels_file to \"labels.txt\" path\n",
    "- Store the labels in a Pandas Data frame called **Labels_Data_Frame**\n",
    "- labels.txtのパスにimport_raw_labels_fileを適用する。\n",
    "- ラベルをPandasのデータフレーム **Labels_Data_Frame** に格納する。<br>\n",
    "<br>\n",
    "<br>\n",
    "experiment: １ファイルの内容（加速度，ジャイロ両方ともの内容について） <br>\n",
    "activity:   行動 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list contains columns names of \"labels.txt\" in order\n",
    "raw_labels_columns=['experiment_number_ID','user_number_ID','activity_number_ID','Label_start_point','Label_end_point']\n",
    "\n",
    "# The path of \"labels.txt\" is last element in the list called \"Raw_data_paths\"\n",
    "labels_path=Raw_data_paths[-1]\n",
    "\n",
    "# apply the function defined above to labels.txt \n",
    "# store the output  in a dataframe \n",
    "Labels_Data_Frame=import_labels_file(labels_path, raw_labels_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 3 rows of labels dataframe\n",
    "print (\"The first 3 rows of  Labels_Data_Frame:\" )\n",
    "display(Labels_Data_Frame.head(3))\n",
    "print(Labels_Data_Frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step16'></a>\n",
    "### I.6. Define Activity Labels Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for all types of activities\n",
    "# The first 6 activities are called Basic Activities as(BAs) 3 dynamic and 3 static\n",
    "# The last 6 activities are called Postural Transitions Activities as (PTAs)\n",
    "Acitivity_labels=AL={\n",
    "        1: 'WALKING', 2: 'WALKING_UPSTAIRS', 3: 'WALKING_DOWNSTAIRS', # 3 dynamic activities\n",
    "        4: 'SITTING', 5: 'STANDING', 6: 'LIYING', # 3 static activities\n",
    "        \n",
    "        7: 'STAND_TO_SIT',  8: 'SIT_TO_STAND',  9: 'SIT_TO_LIE', 10: 'LIE_TO_SIT', \n",
    "    11: 'STAND_TO_LIE', 12: 'LIE_TO_STAND',# 6 postural Transitions\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## II. RawData Exploration\n",
    "\n",
    "### [**II.1. General visualizations**](#step22)\n",
    "\n",
    "### [**II.2. Detailed Visualization**](#step23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step22'></a>\n",
    "### II.1 General visualizations\n",
    "\n",
    "**Useful Rows:** Rows having actually an activity ID<br>\n",
    "        - some rows were captured while the user was not performing the activity protocol these rows considered as not useful in this part.<br>\n",
    "<br>\n",
    "<br>\n",
    "**有用な行:** 実際にアクティビティIDを持つ行<br>\n",
    "        - ユーザがアクティビティプロトコルを実行していない間にキャプチャされた行もあり、これらの行はこの部分では有用でないと考えられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each acc file and gyro file having the same exp_ID have also the same number of rows\n",
    "\n",
    "# a list contains the number of rows per dataframe \n",
    "rows_per_df=[len(raw_dic[key]) for key in sorted(raw_dic.keys())]\n",
    "\n",
    "# a list contains exp ids (experiment_number_ID)\n",
    "exp_ids=[i for i in range(1,62)]\n",
    "\n",
    "# useful row is row that was captured while the user was performing an activity\n",
    "# some rows in acc and gyro files are not associated to an activity id\n",
    "\n",
    "# list that will contain the number of useful rows per dataframe\n",
    "# データフレームごとの有用な行数が格納されるリスト\n",
    "useful_rows_per_df=[]\n",
    "\n",
    "for i in range(1,62):# iterating over exp ids\n",
    "    # selecting start-end rows of each activity of the experiment\n",
    "    start_end_df= Labels_Data_Frame[Labels_Data_Frame['experiment_number_ID']==i][['Label_start_point','Label_end_point']]\n",
    "    # sum of start_labels and sum of end_labels\n",
    "    start_sum,end_sum=start_end_df.sum()\n",
    "    # number of rows useful rows in [exp i] dataframe\n",
    "    useful_rows_number=end_sum-start_sum+len(start_end_df)\n",
    "    # storing row numbers in a list\n",
    "    useful_rows_per_df.append(useful_rows_number)\n",
    "    \n",
    "print(useful_rows_per_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step222'></a>\n",
    "III.1.1. Number of rows per experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing row numbers per dataframe\n",
    "\n",
    "plt.bar(exp_ids,rows_per_df) # ploting the bar plot\n",
    "\n",
    "plt.xlabel('experience identifiers') # Set X axis info\n",
    "plt.ylabel('number of rows') # Set Y axis info\n",
    "plt.title('number of rows per experience') # Set the title of the bar plot\n",
    "plt.show() # Show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II.1.2. Number of useful rows per experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing useful rows per dataframe\n",
    "\n",
    "plt.bar(exp_ids,useful_rows_per_df) # ploting the bar plot\n",
    "plt.xlabel('experience identifiers') # Set X axis info\n",
    "plt.ylabel('number of useful rows') # Set y axis info\n",
    "plt.title('number of useful rows per experience') # Set the title of the figure\n",
    "plt.show() # show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detrimining Number of rows and Mean time per each activity<br>\n",
    "行数および各アクティビティの平均時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list will contain number of rows per activity\n",
    "rows_per_activity=[]\n",
    "\n",
    "# a list will contain the number of times each activity was performed in the protocol of all experiences\n",
    "count_act=[]\n",
    "\n",
    "for i in range(1,13):# iterating over activity ids（全部でアクティビティが12個）\n",
    "    \n",
    "    # a dataframe contains start and end labels for all experiences while users were performing the same activity\n",
    "    start_end_df =Labels_Data_Frame[Labels_Data_Frame['activity_number_ID']==i][['Label_start_point','Label_end_point']]\n",
    "    \n",
    "    # add to the list the number of times this activity was performed in all experiences\n",
    "    # 同じ行動の出現回数\n",
    "    count_act.append(len(start_end_df))\n",
    "    \n",
    "    # start_sum is the sum of all start_label values in start_end_df\n",
    "    # end_sum is the sum of all end_label values in start_end_df\n",
    "    start_sum,end_sum=start_end_df.sum()\n",
    "    \n",
    "    # number of rows related to the activity\n",
    "    number_of_rows=end_sum-start_sum+len(start_end_df)\n",
    "    \n",
    "    # storing number of rows in a list\n",
    "    rows_per_activity.append(number_of_rows)\n",
    "\n",
    "\n",
    "# mean duration in seconds of each activity:\n",
    "# 同様の行動をした行数の合計を，50[hz]に同じ行動の出現回数をかけたもので割れば，各行動の平均時間を算出できる\n",
    "time_per_activity=[rows_per_activity[i]/(float(50)*count_act[i]) for i in range(len(rows_per_activity))]\n",
    "\n",
    "# activity ids from 1 to 12\n",
    "activity_ids=[i for i in range(1,13)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.1.3 Number of rows per activity:**<br>\n",
    "    * In this visualzition we calculate the number of rows per activity using all experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the number of rows per activity\n",
    "plt.bar(activity_ids,rows_per_activity) # plot bar's figure\n",
    "plt.xlabel('activity identifiers') # Set x axis info\n",
    "plt.ylabel('number of rows') # Set y axis info\n",
    "plt.title('number of rows per activity') # set the figure's title\n",
    "plt.show() # showing the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II.1.4. Mean time per activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(activity_ids,time_per_activity)# plot bar's figure\n",
    "plt.xlabel('activity identifiers') # Set x axis info\n",
    "plt.ylabel('time in seconds') # Set y axis info\n",
    "plt.title('mean time associated to each activity ') # Set the figure's title\n",
    "plt.show() # showing the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step23'></a>\n",
    "## II.2. Detailed Visualizations \n",
    "\n",
    "- [**II.2.1. Define visualize_triaxial_signals (visualization function)**](#step233)\n",
    "\n",
    "- [**II.2.2. Visualize acc and gyro signals for two experiences** ](#step234) \n",
    "\n",
    "- [**II.2.3. Define a look up function to explore labels file**](#step236)\n",
    "\n",
    "- [**II.2.4. Display some rows in labels file**](#step237) \n",
    "\n",
    "- [**II.2.5. Visualize signals related to Basic Activities for sample 2**](#step239)\n",
    "\n",
    "- [**II.2.6. Visualize signals related to Postural Transitions for sample 2**](#step241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Info**<br>\n",
    "サンプルとして2つのデータフレームを選択（2つ目のデータフレームはランダムに選択）。\n",
    "   - 両方のデータフレームについて、グローバルですべての加速度信号とジャイロ信号を可視化する。\n",
    "   - 片方のデータフレームで、いくつかの固定アクティビティに関連する信号を可視化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step232'></a>\n",
    "- **Step 0: Selecting two samples**   \n",
    "    * samples keys are:'exp01_user01' and 'exp47_user23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two full samples:\n",
    "sample01_01 = raw_dic['exp01_user01'] # acc and gyro signals of exp 01 user 01\n",
    "sample47_23 = raw_dic['exp47_user23'] # acc and gyro signals of exp 47 user 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='step233'></a>\n",
    "### II.2.1. Define *visualize_triaxial_signals* (visualization function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリング周波数\n",
    "sampling_freq=50 # 50 Hz(hertz) is sampling frequency: the number of captured values of each axial signal per second.\n",
    "\n",
    "\n",
    "# ３軸の信号を表示関数（acc or gyro）\n",
    "def visualize_triaxial_signals(data_frame,exp_id,act,sig_type,width,height):\n",
    "    \n",
    "    \n",
    "    # inputs: Data_frame: Data frame contains acc and gyro signals                                                 #\n",
    "    #         exp_id: integer from 1 to 61 (the experience identifier)                                             #         \n",
    "    #         width: integer the width of the figure                                                               #\n",
    "    #         height: integer the height of the figure                                                             #\n",
    "    #         sig_type: string  'acc' to visualize 3-axial acceleration signals or 'gyro' for 3-axial gyro signals #\n",
    "    #         act: possible values: string: 'all' (to visualize full signals) ,                                    #\n",
    "    #              or integer from 1 to 12 to specify the activity id to be visualized                             #\n",
    "    #                                                                                                              #\n",
    "    #              if act is from 1 to 6 it will skip the first 250 rows(first 5 seconds) from                     #\n",
    "    #              the starting point of the activity and will visualize the next 400 rows (next 8 seconds)        #\n",
    "    #              if act is between 7 and 12  the function will visualize all rows(full duration) of the activity.# \n",
    "    \n",
    "    '''\n",
    "    actが1から6の場合、活動の開始点から最初の250行（最初の5秒）をスキップし、次の400行（次の8秒）を視覚化します。\n",
    "    actが7から12の場合、この関数は活動の全行（全時間）を視覚化します。\n",
    "    （時間はf=50[Hz]より算出）\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    keys=sorted(raw_dic.keys()) # list contains 'expXX_userYY' sorted from 1 to 61\n",
    "    key=keys[exp_id-1] # the key associated to exp_id (experience)\n",
    "    exp_id=str(exp_id)\n",
    "    # key値のうち，ユーザのみ取り出す\n",
    "    user_id=key[-2:] # the user id associated to this experience in string format\n",
    "    \n",
    "    if act=='all': # to visualize full signal\n",
    "        \n",
    "        # selecting all rows in the dataframe to be visualized, \n",
    "        # the dataframe stored in raw_dic and has the same key\n",
    "        data_df=data_frame.copy()\n",
    "        \n",
    "    else:# act is an integer from 1 to 12 (id of the activity to be visualized ) \n",
    "        \n",
    "        # Select rows in labels file having the same exp_Id and user_Id mentioned above + the activity id (act)\n",
    "        # selecting the first result in the search made in labels file\n",
    "        # and select the start point and end point of this row related to this activity Id (act) \n",
    "        start_point,end_point=Labels_Data_Frame[\n",
    "                             (Labels_Data_Frame[\"experiment_number_ID\"]==int(exp_id))&\n",
    "                             (Labels_Data_Frame[\"user_number_ID\"]==int(user_id))&\n",
    "                             (Labels_Data_Frame[\"activity_number_ID\"]==act)\n",
    "                            ][['Label_start_point','Label_end_point']].iloc[0] # ラベルファイル内で行われた検索で最初の結果を選択する\n",
    "        \n",
    "        # 活動ID（act）に関連するこの行の開始点と終了点を選択\n",
    "        if int(act) in [1,2,3,4,5,6]:# if the activity to be visualed is from 1 to 6 (basic activity)\n",
    "            # skip the first 250 rows(5 second)\n",
    "            start_point=start_point+250\n",
    "            \n",
    "            # set the end point at distance of 400 rows (8seconds) from the start_point\n",
    "            end_point=start_point+400\n",
    "    \n",
    "        \n",
    "        # select the 8 seconds shifted by 5 seconds if act between 1 and 6 \n",
    "        # if act is between 7 and 12 select the full duration of the first result(row)\n",
    "        # 表示データを選択\n",
    "        data_df=data_frame[start_point:end_point].copy()\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    \n",
    "    # a list contain all column names of the  (6 columns in total)\n",
    "    columns=data_df.columns\n",
    "\n",
    "    if sig_type=='acc':# if the columns to be visualized are acceleration columns\n",
    "        \n",
    "        # acceleration columns are the first 3 columns acc_X, acc_Y and acc_Z\n",
    "        X_component=data_df[columns[0]] # copy acc_X\n",
    "        Y_component=data_df[columns[1]] # copy acc_Y\n",
    "        Z_component=data_df[columns[2]] # copy acc_Z\n",
    "        \n",
    "        # accelerations legends\n",
    "        legend_X='acc_X'\n",
    "        legend_Y='acc_Y'\n",
    "        legend_Z='acc_Z'\n",
    "\n",
    "        # the figure y axis info\n",
    "        figure_Ylabel='Acceleration in 1g'\n",
    "        \n",
    "        # select the right title in each case\n",
    "        \n",
    "        if act=='all':\n",
    "            title=\"acceleration signals for all activities performed by user \"+ user_id +' in experience '+exp_id\n",
    "        \n",
    "        elif act in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "            title=\"acceleration signals of experience \" + exp_id + \" while user \"+ user_id +' was performing activity: '+str(act)+ '('+AL[act]+')'\n",
    "\n",
    "    elif sig_type=='gyro':# if the columns to be visualized are gyro columns\n",
    "        \n",
    "        # gyro columns are the last 3 columns gyro_X, gyro_Y and gyro_Z\n",
    "        X_component=data_df[columns[3]] # copy gyro_X\n",
    "        Y_component=data_df[columns[4]] # copy gyro_Y\n",
    "        Z_component=data_df[columns[5]] # copy gyro_Z\n",
    "        \n",
    "        # gyro signals legends\n",
    "        legend_X='gyro_X'\n",
    "        legend_Y='gyro_Y'\n",
    "        legend_Z='gyro_Z'\n",
    "        \n",
    "        #the figure y axis info\n",
    "        figure_Ylabel='Angular Velocity in radian per second [rad/s]'\n",
    "        \n",
    "        # select the right title in each case \n",
    "        if act=='all':\n",
    "            title=\"gyroscope signals for all activities performed by user \"+ user_id +' in experience '+exp_id\n",
    "        elif act in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "            title=\"gyroscope signals of experience \"+ exp_id+\" while user \"+ user_id +' was performing activity: '+str(act)+ '('+AL[act]+')'\n",
    "\n",
    "    # chosing colors : red for X component blue for Y component and green for Z component\n",
    "    colors=['r','b','g']\n",
    "    # number of rows in this dataframe to be visualized(depends on 'act' variable)\n",
    "    len_df=len(data_df)\n",
    "    \n",
    "    # converting row numbers into time duration (the duration between two rows is 1/50=0.02 second)\n",
    "    # 横軸の点のリスト\n",
    "    time=[1/float(sampling_freq) * j for j in range(len_df)]\n",
    "    \n",
    "    # Define the figure and setting dimensions width and height\n",
    "    fig = plt.figure(figsize=(width,height))\n",
    "    \n",
    "    # ploting each signal component\n",
    "    _ =plt.plot(time,X_component,color='r',label=legend_X)\n",
    "    _ =plt.plot(time,Y_component,color='b',label=legend_Y)\n",
    "    _ =plt.plot(time,Z_component,color='g',label=legend_Z)\n",
    "    \n",
    "    # Set the figure info defined earlier\n",
    "    _ = plt.ylabel(figure_Ylabel) # set Y axis info \n",
    "    _ = plt.xlabel('Time in seconds (s)') # Set X axis info (same label in all cases)\n",
    "    _ = plt.title(title) # Set the title of the figure\n",
    "    \n",
    "    # localise the figure's legends\n",
    "    _ = plt.legend(loc=\"upper left\")# upper left corner\n",
    "    \n",
    "    # showing the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step234'></a>\n",
    "### II.2.2.  Visualize acc and gyro signals for both samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **1.** Visualzing acceleration signals for all activities of experience 01\n",
    "* **2.** Visualzing gyroscope signals for all activities of experience 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# plotting acc signals for the first sample ######################\n",
    "# figure parameters : width=18 height=5\n",
    "# exp_id=1\n",
    "#                          DataFrame  , exp_Id, act , sig_type  ,Width,height\n",
    "visualize_triaxial_signals(sample01_01,   1   ,'all',    'acc'  ,  18 ,  5   )  \n",
    "# sig_type='acc' to visulize acceleration signals\n",
    "# act='all' to visualize full duration of the dataframe\n",
    "\n",
    "\n",
    "\n",
    "################# plotting gyro signals for the first sample ######################\n",
    "# figure parameters : width=18 height=5\n",
    "# exp_id=1\n",
    "# act='all' to visualize full duration of the dataframe\n",
    "visualize_triaxial_signals(sample01_01,1,'all','gyro',18,5) \n",
    "# sig_type='gyro' to visualize gyro signals\n",
    "# act='all' to visualize full duration of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **3.** Visualzing acceleration signals for all activities of experience 47\n",
    "* **4.** Visualzing gyroscope signals for all activities of experience 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# plotting acc signals for the second sample ######################\n",
    "# figure parameters : width=18 height=5\n",
    "# exp_id=47 user_id=23\n",
    "# act='all' to visualize full duration of the dataframe\n",
    "visualize_triaxial_signals(sample47_23,47,'all','acc',18,5) # sig_type='acc' to visulize acceleration signals\n",
    "\n",
    "\n",
    "################# plotting gyro signals for the second sample ######################\n",
    "# figure parameters : width=18 height=5\n",
    "# exp_id=47 user_id=23\n",
    "# act='all' to visualize full duration of the dataframe\n",
    "visualize_triaxial_signals(sample47_23,47,'all','gyro',18,5) # sig_type='gyro' to visualize gyro signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='step236'></a>\n",
    "### II.2.3. Define a look up function to explore labels file\n",
    "* The look up function generates start and end labels points of activity ID in a dataframe **expXX_userYY**\n",
    "* ルックアップ機能により、アクティビティIDの開始ラベルポイント、終了ラベルポイントをデータフレームexpXX_userYYに生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################FUNCTION: look_up(exp_ID,user_ID,activity_ID)#########################\n",
    "\n",
    "\n",
    "def look_up(exp_ID,activity_ID):\n",
    "    \n",
    "    # Inputs:                                                                            #\n",
    "    #   exp_ID  : integer , the experiment Identifier from 1 to 61 (61 included)         #\n",
    "    #                                                                                    #\n",
    "    #   activity_ID: integer  the activity Identifier from 1 to 12 (12 included)         #\n",
    "    # Outputs:                                                                           #\n",
    "    #   dataframe: A pandas Dataframe which is a part of Labels_Data_Frame contains      #\n",
    "    #             the activity ID ,the start point  and the end point  of this activity  #\n",
    "    \n",
    "    user_ID=int(sorted(raw_dic.keys())[exp_ID -1][-2:])\n",
    "    \n",
    "    # To select rows in labels file of a fixed activity in a fixed experiment \n",
    "    return Labels_Data_Frame[\n",
    "                             (Labels_Data_Frame[\"experiment_number_ID\"]==exp_ID)&\n",
    "                             (Labels_Data_Frame[\"user_number_ID\"]==user_ID)&\n",
    "                             (Labels_Data_Frame[\"activity_number_ID\"]==activity_ID)\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step237'></a>\n",
    "### II.2.4. Display some rows in labels file using the look up function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity_Id in range(1,13):# iterating throw activity ids from 1 to 12\n",
    "    # expID=47 （固定）\n",
    "    # It returns all Label_start_point and Label_end_point of this (activityID,expID)\n",
    "    print('Activity number '+str(activity_Id))\n",
    "    display(look_up(47 ,activity_Id)) # display the results of each search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の結果から、以下のように思われます。\n",
    "    - 動的活動2回（活動02：階段を上る**、活動03：階段を下りる**）が3回実施された。\n",
    "    - 4つの基本アクティビティ（1つの動的アクティビティと3つの静的アクティビティ）を2回実施（アクティビティ01：**歩く**、アクティビティ04：**座る**、アクティビティ05：**立つ**、アクティビティ06：**横たわる**）した。\n",
    "    - 7から12までのすべての姿勢移行アクティビティIDをそれぞれ1回ずつ実施した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step239'></a>\n",
    "### II.2.5. Visualize signals related to *Basic Activities* for sample N° 2\n",
    "\n",
    "\n",
    "* In this detailed visualization i choosed to :\n",
    "    - skip the first 5 seconds (the first 250 row) after the Label_start_point and visualize the next 8 seconds (400 row) of each basic activity using the first search results in each table above.\n",
    "    - For postural transitions i decide to visualize all their full duration.\n",
    "<br>\n",
    "* この詳細な可視化では、以下を選択した。\n",
    "    - Label_start_pointの後の最初の5秒（最初の250行）をスキップし、上記の各テーブルの最初の検索結果を使用して、各基本的活動の次の8秒（400行）を視覚化します。\n",
    "    - 姿勢の遷移については、その全時間を可視化することにしました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Basic activities from 1 to 6\n",
    "for act in range(1,7): # Iterating throw each activity Id from 1 to 6\n",
    "    visualize_triaxial_signals(sample47_23,47,act,'acc',14,2) # visualize acc signals related to this activity\n",
    "    visualize_triaxial_signals(sample47_23,47,act,'gyro',14,2) # visualize gyro signals reated to this activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize postural transitions (act_id from 7 to 12)\n",
    "for activity_Id in range(7,13): # iterating throw each activity Ids from 7 to 12\n",
    "    visualize_triaxial_signals(sample47_23,47,activity_Id,'acc',14,2) # visualize acc signals related to this activity\n",
    "    visualize_triaxial_signals(sample47_23,47,activity_Id,'gyro',14,2) # visualize gyro signals related to this activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Signal Processing PipeLine\n",
    "\n",
    "### [**III.1. Generating time domain signals**](#step32)\n",
    "\n",
    "### [**III.2. Windowing**](#step33) \n",
    "\n",
    "### [**III.3. Features Generation**](#step34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1 Generating time domain signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0:** Define visualize_signal (visualization function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 信号の可視化\n",
    "def visualize_signal(signal,x_labels,y_labels,title,legend):\n",
    "    # Inputs: signal: 1D column \n",
    "    #         x_labels: the X axis info (figure)\n",
    "    #         y_labels: the Y axis info (figure)\n",
    "    #         title: figure's title\n",
    "    #         legend : figure's legend\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the figure's dimensions\n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    # convert row numbers in time durations\n",
    "    time=[1/float(sampling_freq) *i for i in range(len(signal))]\n",
    "    \n",
    "    # plotting the signal\n",
    "    plt.plot(time,signal,label=legend) # plot the signal and add the legend\n",
    "    \n",
    "    plt.xlabel(x_labels) # set the label of x axis in the figure\n",
    "    plt.ylabel(y_labels) # set the label of y axis in the figure\n",
    "    plt.title(title) # set the title of the figure\n",
    "    plt.legend(loc=\"upper left\") # set the legend in the upper left corner\n",
    "    plt.show() # show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step323'></a>\n",
    "### III.1.1. Median Filtering (3rd order)\n",
    "Step 1: Define Median Filter 3rd Order Funtion<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last value is not changed \n",
    "# The first value is the median of signal[0:3]\n",
    "# The second value is the median of signal[0:3]\n",
    "# The third value is the median of signal[1:4]\n",
    "\n",
    "# The value before the last is the median of signal[-3:]\n",
    "# The last value is last value in the original signal\n",
    "\n",
    "# Median Filter: was applied to reduce background noise.\n",
    "from scipy.signal import medfilt # import the median filter function\n",
    "\n",
    "def median(signal):# input: numpy array 1D (one column)\n",
    "    \n",
    "    array=np.array(signal)\n",
    "    \n",
    "    # applying the median filter order3(kernel_size=3)\n",
    "    med_filtered=medfilt(array, kernel_size=3)\n",
    "    \n",
    "    return  med_filtered # return the med-filtered signal: numpy array 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Apply the Median Filter and visualize its effects on one signal<br>\n",
    "（Median Filter: was applied to reduce background noise.）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the filter to see its effects on signals\n",
    "\n",
    "# selecting the signal to be filtered in this case :exp01_user01 acc_X\n",
    "signal_sample=np.array(sample01_01['acc_X']) \n",
    "\n",
    "# generating the filtered signal（フィルタの適用）\n",
    "med_filtred_signal=median(signal_sample)\n",
    "\n",
    "# set figures' legends, x labels, y labels and titles\n",
    "legend1='original exp01_user01 acc_X'\n",
    "legend2='med_filterd exp01_user01 acc_X'\n",
    "x_labels='time in seconds'\n",
    "y_labels='acceleration amplitude in 1g'\n",
    "title1='8 seconds of the original signal'\n",
    "title2='the same 8 seconds after applying the median filter order 3'\n",
    "\n",
    "# skip the first 500 rows (10 seconds) visualize the next 400 rows (8 seconds) for both signals (the original and the filtred ones)\n",
    "visualize_signal(signal_sample[500:900],x_labels,y_labels,title1,legend1) \n",
    "visualize_signal(med_filtred_signal[500:900],x_labels,y_labels,title2,legend2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step324'></a>       \n",
    "### III.1.2 Useful Components Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define components_selection_one_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary functions\n",
    "\n",
    "# import fft(Fast Fourier Transform) function to convert a signal from time domain to \n",
    "# frequency domain (output :is a numpy array contains signal's amplitudes of each frequency component)\n",
    "# 高速フーリエ変換\n",
    "from scipy.fftpack import fft   \n",
    "\n",
    "# import fftfreq function to generate frequencies related to frequency components mentioned above\n",
    "# fftの周波数成分に関連する周波数を生成する機能\n",
    "from scipy.fftpack import fftfreq\n",
    "\n",
    "# import ifft function (inverse fft) inverse the conversion\n",
    "# 高速フーリエ逆変換\n",
    "from scipy.fftpack import ifft\n",
    "\n",
    "# import math library\n",
    "import math\n",
    "\n",
    "############################## Constants #############################\n",
    "\n",
    "# ナイキスト周波数：サンプリング周波数の1/2に相当する周波数\n",
    "nyq=sampling_freq/float(2) # nyq is the nyquist frequency equal to the half of the sampling frequency[50/2= 25 Hz]\n",
    "\n",
    "# カットオフ周波数\n",
    "freq1 = 0.3 # freq1=0.3 hertz [Hz] the cuttoff frequency between the DC compoenents [0,0.3]\n",
    "#           and the body components[0.3,20]hz\n",
    "freq2 = 20  # freq2= 20 Hz the cuttoff frequcency between the body components[0.3,20] hz\n",
    "#             and the high frequency noise components [20,25] hz\n",
    "\n",
    "\n",
    "\n",
    "# Function name: components_selection_one_signal\n",
    "\n",
    "# Inputs: t_signal:1D numpy array (time domain signal); \n",
    "\n",
    "# Outputs: (total_component,t_DC_component , t_body_component, t_noise) \n",
    "#           type(1D array,1D array, 1D array)\n",
    "\n",
    "# cases to discuss: if the t_signal is an acceleration signal then the t_DC_component is the gravity component [Grav_acc]\n",
    "#                   if the t_signal is a gyro signal then the t_DC_component is not useful\n",
    "# t_noise component is not useful\n",
    "# if the t_signal is an acceleration signal then the t_body_component is the body's acceleration component [Body_acc]\n",
    "# if the t_signal is a gyro signal then the t_body_component is the body's angular velocity component [Body_gyro]\n",
    "\n",
    "def components_selection_one_signal(t_signal,freq1,freq2):\n",
    "    \n",
    "    t_signal=np.array(t_signal)\n",
    "    t_signal_length=len(t_signal) # number of points in a t_signal\n",
    "    \n",
    "    # the t_signal in frequency domain after applying fft\n",
    "    f_signal=fft(t_signal) # 1D numpy array contains complex values (in C)\n",
    "    \n",
    "    # generate frequencies associated to f_signal complex values\n",
    "    freqs=np.array(sp.fftpack.fftfreq(t_signal_length, d=1/float(sampling_freq))) # frequency values between [-25hz:+25hz]\n",
    "    \n",
    "    # DC_component: f_signal values having freq between [-0.3 hz to 0 hz] and from [0 hz to 0.3hz] \n",
    "    #                                                             (-0.3 and 0.3 are included)\n",
    "    \n",
    "    # noise components: f_signal values having freq between [-25 hz to 20 hz[ and from ] 20 hz to 25 hz] \n",
    "    #                                                               (-25 and 25 hz inculded 20hz and -20hz not included)\n",
    "    \n",
    "    # selecting body_component: f_signal values having freq between [-20 hz to -0.3 hz] and from [0.3 hz to 20 hz] \n",
    "    #                                                               (-0.3 and 0.3 not included , -20hz and 20 hz included)\n",
    "    \n",
    "    \n",
    "    f_DC_signal=[] # DC_component in freq domain\n",
    "    f_body_signal=[] # body component in freq domain numpy.append(a, a[0])\n",
    "    f_noise_signal=[] # noise in freq domain\n",
    "    \n",
    "    for i in range(len(freqs)):# iterate over all available frequencies\n",
    "        \n",
    "        # selecting the frequency value\n",
    "        freq=freqs[i]\n",
    "        \n",
    "        # selecting the f_signal value associated to freq\n",
    "        value= f_signal[i]\n",
    "        \n",
    "        # Selecting DC_component values \n",
    "        if abs(freq)>0.3:# testing if freq is outside DC_component frequency ranges\n",
    "            f_DC_signal.append(float(0)) # add 0 to  the  list if it was the case (the value should not be added)                                       \n",
    "        else: # if freq is inside DC_component frequency ranges \n",
    "            f_DC_signal.append(value) # add f_signal value to f_DC_signal list\n",
    "    \n",
    "        # Selecting noise component values \n",
    "        if (abs(freq)<=20):# testing if freq is outside noise frequency ranges \n",
    "            f_noise_signal.append(float(0)) # # add 0 to  f_noise_signal list if it was the case \n",
    "        else:# if freq is inside noise frequency ranges \n",
    "            f_noise_signal.append(value) # add f_signal value to f_noise_signal\n",
    "\n",
    "        # Selecting body_component values \n",
    "        if (abs(freq)<=0.3 or abs(freq)>20):# testing if freq is outside Body_component frequency ranges\n",
    "            f_body_signal.append(float(0))# add 0 to  f_body_signal list\n",
    "        else:# if freq is inside Body_component frequency ranges\n",
    "            f_body_signal.append(value) # add f_signal value to f_body_signal list\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################### Inverse the transformation of signals in freq domain ########################\n",
    "    ################################### 逆フーリエ変換 ################################################\n",
    "    \n",
    "    # applying the inverse fft(ifft) to signals in freq domain and put them in float format\n",
    "    t_DC_component= ifft(np.array(f_DC_signal)).real\n",
    "    t_body_component= ifft(np.array(f_body_signal)).real\n",
    "    t_noise=ifft(np.array(f_noise_signal)).real\n",
    "    \n",
    "    total_component=t_signal-t_noise # extracting the total component(filtered from noise) \n",
    "                                     #  by substracting noise from t_signal (the original signal).\n",
    "    \n",
    "    # return outputs mentioned earlier\n",
    "    return (total_component,t_DC_component,t_body_component,t_noise) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Test it on a sample<br>\n",
    "Step 3: Verify Gravity Magnitudes\n",
    "- [ Define verify gravity function](#step32442) \n",
    "- [Gravity visualization](#step32443)\n",
    "\n",
    "このフィルタリング関数が正しいかどうかは、次のようにして確認することができる。\n",
    "- 重力ユークリッドの大きさ grav_acc_mag を [grav_acc_X,grav_acc_Y,grav_acc_Z] で可視化する。\n",
    "- 加速度の単位が g であるため、重力の大きさは 1 に近いはずである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step32442'></a>\n",
    "1. Define verify gravity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Euclidian magnitude\n",
    "# 加速度の合成\n",
    "def mag_3_signals(x,y,z):\n",
    "    return [math.sqrt((x[i]**2+y[i]**2+z[i]**2)) for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_gravity(exp_id):\n",
    "    # inputs: exp_id integer from 1 to 61\n",
    "    \n",
    "    keys=sorted(raw_dic.keys())# expXX_userYY ordered from 1 to 61\n",
    "    key=keys[exp_id-1]# select the key associated to exp_id\n",
    "    \n",
    "    # 信号を取り出す\n",
    "    acc_x=np.array(raw_dic[key]['acc_X']) # copy acc_X column from dataframe in raw_dic having the key mentioned above\n",
    "    acc_y=np.array(raw_dic[key]['acc_Y'])# copy acc_Y column  from dataframe in raw_dic having the key mentioned above\n",
    "    acc_z=np.array(raw_dic[key]['acc_Z'])# copy acc_Z column  from dataframe in raw_dic having the key mentioned above\n",
    "\n",
    "    # apply the filtering method to acc_[X,Y,Z] and store gravity components\n",
    "    grav_acc_X=components_selection_one_signal(acc_x,freq1,freq2)[1] \n",
    "    grav_acc_Y=components_selection_one_signal(acc_y,freq1,freq2)[1]\n",
    "    grav_acc_Z=components_selection_one_signal(acc_z,freq1,freq2)[1]\n",
    "    \n",
    "    # calculating gravity magnitude signal\n",
    "    grav_acc_mag=mag_3_signals(grav_acc_X, grav_acc_Y,grav_acc_Z)\n",
    "    print(len(grav_acc_mag))\n",
    "\n",
    "    x_labels='time in seconds' # Set x axis info\n",
    "    # 1g単位での重力振幅 \n",
    "    y_labels='gravity amplitude in 1g' # set y axis info\n",
    "    title='the euclidian magnitude of gravity 3-axial signals' # set the figure's title\n",
    "    legend=key+' grav_acc_mag' # set the figure's legend\n",
    "    \n",
    "    visualize_signal(grav_acc_mag,x_labels,y_labels,title,legend) # visualize gravity magnitude signal\n",
    "    \n",
    "    # 有効数字5ケタで表示\n",
    "    print('mean value = '+str(np.array(grav_acc_mag).mean())[0:5]+ ' g') # print the gravity magnitude mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step32443'></a>\n",
    "2. Gravity Magnitude visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to dataframe related to \" experience 01 \"\n",
    "verify_gravity(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3. Jerking（加加速度） function & Magnitude function\n",
    "\n",
    "- [**Define jerking and magnitude functions**](#step3252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3252'></a>\n",
    "Define jerking and magnitude functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################Jerk Signals Functions #####################################\n",
    "\n",
    "# d(signal)/dt : the Derivative\n",
    "# jerk(signal(x0)) is equal to (signal(x0+dx)-signal(x0))/dt （加加速度 ＝ 加速度の微分）\n",
    "# Where: signal(x0+dx)=signal[index[x0]+1] and  signal(x0)=signal[index[x0]]\n",
    "\n",
    "\n",
    "dt=0.02 # dt=1/50=0.02s time duration between two rows\n",
    "# Input: 1D array with lenght=N (N:unknown)\n",
    "# Output: 1D array with lenght=N-1\n",
    "\n",
    "# 加速度の微分の近似値を求める\n",
    "def jerk_one_signal(signal): \n",
    "        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Magnitude Function ######################################\n",
    "import math \n",
    "\n",
    "# 加速度の合成\n",
    "def mag_3_signals(x,y,z):# magnitude function redefintion\n",
    "    return np.array([math.sqrt((x[i]**2+y[i]**2+z[i]**2)) for i in range(len(x))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step326'></a>\n",
    "### 3.1.5. Time domain signals generation (important !!)\n",
    "- [**Time domain signals generation PipeLine**](#step3261)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3261'></a>\n",
    "Time domain signals generation PipeLine<br>\n",
    "時間信号の生成パイプライン（時間の特徴量抽出ための信号を生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sig_dic={} # An empty dictionary will contains dataframes of all time domain signals\n",
    "raw_dic_keys=sorted(raw_dic.keys()) # sorting dataframes' keys\n",
    "\n",
    "for key in raw_dic_keys: # iterate over each key in raw_dic\n",
    "    \n",
    "    raw_df=raw_dic[key] # copie the raw dataframe associated to 'expXX_userYY' from raw_dic\n",
    "    \n",
    "    time_sig_df=pd.DataFrame() # a dataframe will contain time domain signals\n",
    "    \n",
    "    for column in raw_df.columns: # iterate over each column in raw_df\n",
    "        \n",
    "        t_signal=np.array(raw_df[column]) # copie the signal values in 1D numpy array\n",
    "        \n",
    "        med_filtred=median(t_signal) # apply 3rd order median filter and store the filtred signal in med_filtred\n",
    "        \n",
    "        if 'acc' in column: # test if the med_filtered signal is an acceleration signal \n",
    "            \n",
    "            # the 2nd output DC_component is \" the gravity_acc \"\"\n",
    "            # The 3rd one is \" the body_component \"  which in this case the body_acc\n",
    "            _,grav_acc,body_acc,_=components_selection_one_signal(med_filtred,freq1,freq2) # apply components selection\n",
    "            \n",
    "            body_acc_jerk=jerk_one_signal(body_acc)# apply the jerking function to body components only\n",
    "            \n",
    "            \n",
    "            # store signal in time_sig_dataframe and delete the last value of each column \n",
    "            # jerked signal will have the original length-1 (due to jerking)\n",
    "            \n",
    "            time_sig_df['t_body_'+column]=body_acc[:-1] # t_body_acc storing with the appropriate axis selected \n",
    "            #                                             from the column name\n",
    "            \n",
    "            time_sig_df['t_grav_'+column]= grav_acc[:-1] # t_grav_acc_storing with the appropriate axis selected \n",
    "            #                                              from the column name\n",
    "            \n",
    "            # store  t_body_acc_jerk signal with the appropriate axis selected from the column name\n",
    "            time_sig_df['t_body_acc_jerk_'+column[-1]]=body_acc_jerk\n",
    "        \n",
    "        elif 'gyro' in column: # if the med_filtred signal is a gyro signal\n",
    "            \n",
    "            # The 3rd output of components_selection is the body_component which in this case the body_gyro component\n",
    "            _,_,body_gyro,_=components_selection_one_signal(med_filtred,freq1,freq2)  # apply components selection\n",
    "            \n",
    "            body_gyro_jerk=jerk_one_signal(body_gyro) # apply the jerking function to body components only\n",
    "            \n",
    "            # store signal in time_sig_dataframe and delete the last value of each column \n",
    "            # jerked signal will have the original lenght-1(due to jerking)\n",
    "            \n",
    "            time_sig_df['t_body_gyro_'+column[-1]]=body_gyro[:-1] # t_body_acc storing with the appropriate axis selected \n",
    "            #                                                       from the column name\n",
    "            \n",
    "            time_sig_df['t_body_gyro_jerk_'+column[-1]]=body_gyro_jerk # t_grav_acc_storing with the appropriate axis \n",
    "            #                                                            selected from the column name\n",
    "    \n",
    "    \n",
    "    # all 15 axial signals generated above are reordered to facilitate magnitudes signals generation\n",
    "    # 上記で生成された全15軸の信号は、マグニチュード信号の生成を容易にするために並べ替えられています。\n",
    "    new_columns_ordered=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z',\n",
    "                          't_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z',\n",
    "                          't_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z',\n",
    "                          't_body_gyro_X','t_body_gyro_Y','t_body_gyro_Z',\n",
    "                          't_body_gyro_jerk_X','t_body_gyro_jerk_Y','t_body_gyro_jerk_Z']\n",
    "    \n",
    "    # create new dataframe to order columns\n",
    "    ordered_time_sig_df=pd.DataFrame()\n",
    "    \n",
    "    for col in new_columns_ordered: # iterate over each column in the new order\n",
    "        ordered_time_sig_df[col]=time_sig_df[col] # store the column in the ordred dataframe\n",
    "    \n",
    "    # Generating magnitude signals\n",
    "    for i in range(0,15,3): # iterating over each 3-axial signals\n",
    "        \n",
    "        mag_col_name=new_columns_ordered[i][:-1]+'mag'# Create the magnitude column name related to each 3-axial signals\n",
    "        \n",
    "        col0=np.array(ordered_time_sig_df[new_columns_ordered[i]]) # copy X_component\n",
    "        col1=ordered_time_sig_df[new_columns_ordered[i+1]] # copy Y_component\n",
    "        col2=ordered_time_sig_df[new_columns_ordered[i+2]] # copy Z_component\n",
    "        \n",
    "        # 合成した重力信号の値を格納\n",
    "        mag_signal=mag_3_signals(col0,col1,col2) # calculate magnitude of each signal[X,Y,Z]\n",
    "        ordered_time_sig_df[mag_col_name]=mag_signal # store the signal_mag with its appropriate column name\n",
    "    \n",
    "    time_sig_dic[key]=ordered_time_sig_df # store the ordred_time_sig_df in time_sig_dic with the appropriate key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp01の内容をtest表示\n",
    "\n",
    "display(time_sig_dic['exp01_user01'].shape) # the of the first dataframe\n",
    "display(time_sig_dic['exp01_user01'].describe()) # dataframe's statistics\n",
    "time_sig_dic['exp01_user01'].head(3) # displaying the fisrt three rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step33'></a>\n",
    "## III.2 Windowing\n",
    "\n",
    "\n",
    "### [**III.2.1. Windowing type 1**](#step333)\n",
    "### [**III.2.2. Windowing type 2**](#step334)\n",
    "### [**III.2.3. Frequency Windows Generation**](#step335)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step332'></a>\n",
    "#### Step 0: Define supplementery functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: 679 ==> '00679'; 50 ==> '00050'\n",
    "\n",
    "# it add '0's to the left of the input until the new lenght is equal to 5\n",
    "def normalize5(number): \n",
    "    stre=str(number)\n",
    "    if len(stre)<5:\n",
    "        l=len(stre)\n",
    "        for i in range(0,5-l):\n",
    "            stre=\"0\"+stre\n",
    "    return stre \n",
    "\n",
    "# it add '0's to the left of the input until the new lenght is equal to 2\n",
    "def normalize2(number):\n",
    "    stre=str(number)\n",
    "    if len(stre)<2:\n",
    "        stre=\"0\"+stre\n",
    "    return stre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step333'></a>\n",
    "### 3.2.1 Windowing type 1\n",
    "#### [**Step 1: Define Windowing type 1 function**](#step3332)\n",
    "#### [**Step 2: Windows type I Generation and Storage**](#step3333)\n",
    "#### [**Display samples & Discussion**](#step3334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: \n",
    "# time_sig_dic: dictionary includes all time domain signal's dataframes\n",
    "# Labels_Data_Frame: labels file defined earlier\n",
    "\n",
    "# アクティビティごとに分ける\n",
    "def Windowing_type_1(time_sig_dic,Labels_Data_Frame):   \n",
    "    \n",
    "    columns=time_sig_dic['exp01_user01'].columns # columns of time_sig_df\n",
    "    window_ID=0 # window unique id\n",
    "    t_dic_win_type_I={} # output dic\n",
    "    \n",
    "    # Just Basic Activities\n",
    "    BA_array=np.array(Labels_Data_Frame[(Labels_Data_Frame[\"activity_number_ID\"] <7)]) \n",
    "    \n",
    "    for line in BA_array:\n",
    "        # Each line in BA_array contains info realted to an activity\n",
    "\n",
    "        # extracting the dataframe key that contains rows related to this activity [expID,userID]\n",
    "        file_key= 'exp' + normalize2(int(line[0]))  +  '_user' + normalize2(int(line[1]))\n",
    "\n",
    "        # extract the activity_ID in this line\n",
    "        act_ID=line[2] # The activity identifier from 1 to 6 (6 included)\n",
    "\n",
    "        # starting point index of an activity\n",
    "        start_point=line[3]\n",
    "\n",
    "        \n",
    "        # ウィンドウサイズ 128行分のデータ\n",
    "        # from the cursor we copy a window that has 128 rows\n",
    "        # the cursor step is 64 data point (50% of overlap) : each time it will be shifted by 64 rows\n",
    "        for cursor in range(start_point,line[4]-127,64):\n",
    "\n",
    "            # end_point: cursor(the first index in the window) + 128\n",
    "            end_point=cursor+128 # window end row\n",
    "\n",
    "            # selecting window data points convert them to numpy array to delete rows index\n",
    "            data=np.array(time_sig_dic[file_key].iloc[cursor:end_point])\n",
    "\n",
    "            # converting numpy array to a dataframe with the same column names\n",
    "            window=pd.DataFrame(data=data,columns=columns)\n",
    "\n",
    "            # creating the window\n",
    "            key='t_W'+normalize5(window_ID)+'_'+file_key+'_act'+normalize2(act_ID)\n",
    "            t_dic_win_type_I[key]=window\n",
    "\n",
    "            # incrementing the windowID by 1\n",
    "            window_ID=window_ID+1\n",
    "        \n",
    "    return t_dic_win_type_I # return a dictionary including time domain windows type I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3333'></a>\n",
    "#### Windows type I Generation and Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the sliding window type 1 to \"time_sig dic\"\n",
    "t_dic_win_type_I  = Windowing_type_1(time_sig_dic,Labels_Data_Frame)\n",
    "print(len(t_dic_win_type_I))\n",
    "print(t_dic_win_type_I.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step334'></a>\n",
    "### III.2.2 Windowing type 2\n",
    "#### [Step 1: Define the Voting function](#step3342)\n",
    "#### [Step 2: Define Windowing type II](#step3344)\n",
    "#### [Step 3: Windows type II Generation and Storage](#step3345)\n",
    "#### [Display Samples & Discussion](#step3346)\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3342'></a>\n",
    "#### Step 1: Define the Voting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The voting Function:\n",
    "\n",
    "# inputs:\n",
    "#         Labels_Data_Frame: Dataframe defined earlier\n",
    "#         exp_ID: the experience id of the dataframe the windowing function type II is dealing with\n",
    "#         Cursor: index of the first row of a window\n",
    "\n",
    "\n",
    "# cursorとend_point間に1番近い行動を探索\n",
    "def find_activity(Labels_Data_Frame,exp_ID,cursor,end_point):\n",
    "    \n",
    "    # array contains all rows from Labels_Data_Frame having related to experience :exp_ID\n",
    "    exp_AL=np.array(Labels_Data_Frame[Labels_Data_Frame['experiment_number_ID']==exp_ID])\n",
    "    \n",
    "    \n",
    "    # cursor前（first）と，end_point後（second）のactibity\n",
    "    # the near starting point above(inferior) the cursor\n",
    "    St_p1=exp_AL[exp_AL[:,3]<=cursor][:,3].max()  # starting point label of the first activity\n",
    "    \n",
    "    # the near ending point below the cursor (superior) \n",
    "    En_p2=exp_AL[exp_AL[:,4]>=end_point][:,4].min() # ending point label of the second activity\n",
    "\n",
    "    for index in range(len(exp_AL)): # iterating over each line \n",
    "        \n",
    "        if exp_AL[index,3]==St_p1: # selecting the row index with starting point \n",
    "            Ind1=index # index of the first activity\n",
    "        \n",
    "        if exp_AL[index,4]==En_p2: # selecting the row index with ending point ==b\n",
    "            Ind2=index # the index of the second activity\n",
    "    \n",
    "    if Ind1 == Ind2 : # if the window rows indexes are inside the same activity data points which means \n",
    "        #                that the first and second activity are actually the same \n",
    "        \n",
    "        activity=exp_AL[Ind1,2] # window_activity id will take the same value as the activity_label of these rows\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # exp_AL[Ind1,4] is the ending_point_label of the first activity\n",
    "        if cursor+63 <= exp_AL[Ind1,4]:# if the first 64 data points or more are included in the first activity bands\n",
    "            activity=exp_AL[Ind1,2] # the window will take the same activity labels as the first activity\n",
    "        \n",
    "        # exp_AL[Ind2,3] is the starting_point_label of the second activity\n",
    "        elif cursor+64 >= exp_AL[Ind2,3]:# if the last 64 data points or more  are included in the second activity bands\n",
    "            activity=exp_AL[Ind2,2] # the window will take the activity labels as the second activity\n",
    "        \n",
    "        else: # if  more than 64 data point doesn't belong (all at once) neither to first activity or to the second activity  \n",
    "            activity=None # this window activity label will be equal to None\n",
    "    \n",
    "    if activity != None:\n",
    "        return normalize2(activity) # to convert to string and add a '0' to the left if activity <10\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3344'></a>\n",
    "#### Step 2: Define Windowing type II<br>\n",
    "experimentにおいて，128行分のデータの中から最適の行動を抽出し，その情報を時間信号に付随させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: \n",
    "# time_sig_dic: dictionary includes all time domain signals' dataframes\n",
    "# Labels_Data_Frame: labels file defined earlier\n",
    "\n",
    "# experimentごとに分ける\n",
    "def Windowing_type_2(time_sig_dic,Labels_Data_Frame):\n",
    "    columns=time_sig_dic['exp01_user01'].columns\n",
    "    window_ID=0\n",
    "    t_dic_win_type_II={}\n",
    "\n",
    "    for key in sorted(time_sig_dic.keys()):\n",
    "\n",
    "        exp_array=np.array(time_sig_dic[key]) # converting the data frame in a an array\n",
    "        lenght_exp=len(exp_array) # number of rows in this array\n",
    "\n",
    "        # Extracting the experience number ID an convert it to int \n",
    "        exp_ID=int(key[3:5])\n",
    "        #user_ID=int(key[10:])\n",
    "\n",
    "        # Selecting Labels_Data_Frame rows blonging to the same experiment ID : exp_ID\n",
    "        exp_AL_array=np.array( Labels_Data_Frame[ Labels_Data_Frame['experiment_number_ID'] == exp_ID])\n",
    "\n",
    "        # The first starting_point_label in this experiment\n",
    "        start_point=exp_AL_array[0,3]\n",
    "\n",
    "        # The last ending_point_label of this experiment\n",
    "        end_point=exp_AL_array[-1,4]\n",
    "\n",
    "        for cursor in range(start_point,end_point-127,64): # the cursor always represents the index of first data point in a window\n",
    "            end_window=cursor+127 # end_window is the index of the last data point in a window\n",
    "\n",
    "            # creating the window\n",
    "            window_array=exp_array[cursor:end_window+1,:]\n",
    "\n",
    "            # Determining the appropriate activity label of this window\n",
    "            act_ID=find_activity(Labels_Data_Frame,exp_ID,cursor,end_window) # strings from 01 to 12 or None\n",
    "\n",
    "            if act_ID!=None: # if act_ID is none  this window doesn't belong to any activity\n",
    "\n",
    "                # since the act_ID is != to None the window array will be stored in DataFrame with the appropriate column names\n",
    "                window_df=pd.DataFrame(data=window_array,columns=columns)\n",
    "\n",
    "                # Generating the window key(unique key :since the window_ID is unique for each window )\n",
    "                # I choosed to add the exp, user, activity Identifiers in the win_key they will be usefull later. (exp: optional)\n",
    "\n",
    "                win_key='t_W'+normalize5(window_ID)+'_'+key+'_act'+str(act_ID)# eg: 'W_00000_exp01_user01_act01'\n",
    "\n",
    "                # Store the window data frame in a dic\n",
    "                t_dic_win_type_II[win_key]=window_df\n",
    "\n",
    "                # Incrementing window_ID by 1\n",
    "                window_ID=window_ID+1\n",
    "    \n",
    "    return t_dic_win_type_II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3345'></a>\n",
    "#### Step 3: Windows type II Generation and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the sliding window type 2 to \"time_sig_dic\"\n",
    "t_dic_win_type_II = Windowing_type_2(time_sig_dic,Labels_Data_Frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='step335'></a>\n",
    "### III.2.3. Frequency Windows Generation\n",
    "#### [**Step 1: Define fast_fourier_transform functions**](#step3352) \n",
    "#### [**Step 2: Apply it to one sample**](#step3353)\n",
    "#### [**Step 3: Generation and Storage of Frequency Windows**](#step3354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3352'></a>\n",
    "####  Step 1: fast_fourier_transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack # import fftpack to use all fft functions\n",
    "from numpy.fft import *\n",
    "\n",
    "# 1つの信号をfftする\n",
    "\n",
    "##################### fast_fourier_transform_one_signal #################\n",
    "# Inputs: time signal 1D array\n",
    "# Output: amplitude of fft components 1D array having the same lenght as the Input\n",
    "def fast_fourier_transform_one_signal(t_signal):\n",
    "    # apply fast fourrier transform to the t_signal\n",
    "    complex_f_signal= fftpack.fft(t_signal) # 複素数\n",
    "    #compute the amplitude each complex number\n",
    "    amplitude_f_signal=np.abs(complex_f_signal) # 複素数の大きさを取る\n",
    "    # return the amplitude\n",
    "    return amplitude_f_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowデータをfftする\n",
    "\n",
    "##################### fast fourier transform for data frames #################\n",
    "# Inputs: A DataFrame with 20 time signal (20 columns) gravity columns(4) won't be transformed\n",
    "# Outputs: A DataFrame with 16 frequency signal (16 columns)\n",
    "def fast_fourier_transform(t_window):\n",
    "    \n",
    "    f_window=pd.DataFrame() # create an empty dataframe will include frequency domain signals of window\n",
    "    \n",
    "    for column in t_window.columns: # iterating over time domain window columns(signals)\n",
    "        \n",
    "        if 'grav' not in column: # verify if time domain signal is not related to gravity components\n",
    "            \n",
    "            t_signal=np.array(t_window[column]) # convert the column to a 1D numpy array\n",
    "           \n",
    "            f_signal= np.apply_along_axis(fast_fourier_transform_one_signal,0,t_signal) # apply the function defined above to the column\n",
    "            \n",
    "            # fft後のwindowデータ\n",
    "            f_window[\"f_\"+column[2:]]=f_signal # storing the frequency signal in f_window with an appropriate column name\n",
    "    \n",
    "    return f_window # return the frequency domain window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3353'></a>\n",
    "####  Step 2: Apply it to one sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating 1 frequency domain window for verfication\n",
    "t_dic_win_type_II['t_W00000_exp01_user01_act05'].pipe(fast_fourier_transform).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3354'></a>\n",
    "####  Step 3: Generation and Storage of Frequency Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries includes f_windows obtained from t_windows type I and type II\n",
    "f_dic_win_type_I = {'f'+key[1:] : t_w1_df.pipe(fast_fourier_transform) for key, t_w1_df in t_dic_win_type_I.items()}\n",
    "f_dic_win_type_II = {'f'+key[1:] : t_w2_df.pipe(fast_fourier_transform) for key, t_w2_df in t_dic_win_type_II.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_body_acc_X</th>\n",
       "      <th>f_body_acc_Y</th>\n",
       "      <th>f_body_acc_Z</th>\n",
       "      <th>f_body_acc_jerk_X</th>\n",
       "      <th>f_body_acc_jerk_Y</th>\n",
       "      <th>f_body_acc_jerk_Z</th>\n",
       "      <th>f_body_gyro_X</th>\n",
       "      <th>f_body_gyro_Y</th>\n",
       "      <th>f_body_gyro_Z</th>\n",
       "      <th>f_body_gyro_jerk_X</th>\n",
       "      <th>f_body_gyro_jerk_Y</th>\n",
       "      <th>f_body_gyro_jerk_Z</th>\n",
       "      <th>f_body_acc_mag</th>\n",
       "      <th>f_body_acc_jerk_mag</th>\n",
       "      <th>f_body_gyro_mag</th>\n",
       "      <th>f_body_gyro_jerk_mag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257558</td>\n",
       "      <td>0.055128</td>\n",
       "      <td>0.568459</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>1.018802</td>\n",
       "      <td>2.057622</td>\n",
       "      <td>1.302252</td>\n",
       "      <td>2.880254</td>\n",
       "      <td>0.066856</td>\n",
       "      <td>0.574643</td>\n",
       "      <td>8.573938</td>\n",
       "      <td>3.442790</td>\n",
       "      <td>3.790846</td>\n",
       "      <td>25.481237</td>\n",
       "      <td>14.048363</td>\n",
       "      <td>73.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.324072</td>\n",
       "      <td>1.169116</td>\n",
       "      <td>2.325911</td>\n",
       "      <td>0.794786</td>\n",
       "      <td>1.852232</td>\n",
       "      <td>4.739415</td>\n",
       "      <td>2.270811</td>\n",
       "      <td>8.786654</td>\n",
       "      <td>3.040972</td>\n",
       "      <td>5.543606</td>\n",
       "      <td>13.054902</td>\n",
       "      <td>4.222888</td>\n",
       "      <td>0.110791</td>\n",
       "      <td>1.628419</td>\n",
       "      <td>2.657498</td>\n",
       "      <td>6.699360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.045438</td>\n",
       "      <td>0.300832</td>\n",
       "      <td>0.599853</td>\n",
       "      <td>0.220647</td>\n",
       "      <td>0.463545</td>\n",
       "      <td>1.179457</td>\n",
       "      <td>0.332364</td>\n",
       "      <td>2.154464</td>\n",
       "      <td>0.845314</td>\n",
       "      <td>1.462829</td>\n",
       "      <td>2.404663</td>\n",
       "      <td>0.720631</td>\n",
       "      <td>0.721155</td>\n",
       "      <td>0.921396</td>\n",
       "      <td>1.619180</td>\n",
       "      <td>3.680039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026573</td>\n",
       "      <td>0.162318</td>\n",
       "      <td>0.341578</td>\n",
       "      <td>0.195753</td>\n",
       "      <td>0.523164</td>\n",
       "      <td>0.458591</td>\n",
       "      <td>0.396876</td>\n",
       "      <td>1.331866</td>\n",
       "      <td>0.486980</td>\n",
       "      <td>2.359180</td>\n",
       "      <td>1.583174</td>\n",
       "      <td>0.965404</td>\n",
       "      <td>0.326605</td>\n",
       "      <td>1.490716</td>\n",
       "      <td>0.289519</td>\n",
       "      <td>2.877012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040355</td>\n",
       "      <td>0.192653</td>\n",
       "      <td>0.218084</td>\n",
       "      <td>0.391329</td>\n",
       "      <td>0.872601</td>\n",
       "      <td>1.024741</td>\n",
       "      <td>0.484334</td>\n",
       "      <td>0.835291</td>\n",
       "      <td>0.432675</td>\n",
       "      <td>4.736053</td>\n",
       "      <td>0.525072</td>\n",
       "      <td>1.014540</td>\n",
       "      <td>0.252132</td>\n",
       "      <td>0.395907</td>\n",
       "      <td>0.434509</td>\n",
       "      <td>0.690402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f_body_acc_X  f_body_acc_Y  f_body_acc_Z  f_body_acc_jerk_X  \\\n",
       "0      0.257558      0.055128      0.568459           0.004474   \n",
       "1      0.324072      1.169116      2.325911           0.794786   \n",
       "2      0.045438      0.300832      0.599853           0.220647   \n",
       "3      0.026573      0.162318      0.341578           0.195753   \n",
       "4      0.040355      0.192653      0.218084           0.391329   \n",
       "\n",
       "   f_body_acc_jerk_Y  f_body_acc_jerk_Z  f_body_gyro_X  f_body_gyro_Y  \\\n",
       "0           1.018802           2.057622       1.302252       2.880254   \n",
       "1           1.852232           4.739415       2.270811       8.786654   \n",
       "2           0.463545           1.179457       0.332364       2.154464   \n",
       "3           0.523164           0.458591       0.396876       1.331866   \n",
       "4           0.872601           1.024741       0.484334       0.835291   \n",
       "\n",
       "   f_body_gyro_Z  f_body_gyro_jerk_X  f_body_gyro_jerk_Y  f_body_gyro_jerk_Z  \\\n",
       "0       0.066856            0.574643            8.573938            3.442790   \n",
       "1       3.040972            5.543606           13.054902            4.222888   \n",
       "2       0.845314            1.462829            2.404663            0.720631   \n",
       "3       0.486980            2.359180            1.583174            0.965404   \n",
       "4       0.432675            4.736053            0.525072            1.014540   \n",
       "\n",
       "   f_body_acc_mag  f_body_acc_jerk_mag  f_body_gyro_mag  f_body_gyro_jerk_mag  \n",
       "0        3.790846            25.481237        14.048363             73.764900  \n",
       "1        0.110791             1.628419         2.657498              6.699360  \n",
       "2        0.721155             0.921396         1.619180              3.680039  \n",
       "3        0.326605             1.490716         0.289519              2.877012  \n",
       "4        0.252132             0.395907         0.434509              0.690402  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the first f_window type I\n",
    "f_window = f_dic_win_type_I[sorted(f_dic_win_type_I.keys())[0]]\n",
    "f_window.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step34'></a>\n",
    "## III.3 Features Generation（特徴量抽出）:\n",
    "\n",
    "### [**III.3.1 Common Features**](#step342)\n",
    "###  [**III.3.2. Time Features**](#step343)\n",
    "### [**III.3.3. Frequency features**](#step344) \n",
    "### [**III.3.4. Additional features**](#step345)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Info**\n",
    "\n",
    "* axial features : features calculated for axial signals X or Y or Z.\n",
    "    - 1 common features: features generated from both time and frequency 3-axial signals using the same methods\n",
    "        - Practically i will use the same features functions to generate time and frequency features from axial signals\n",
    "    - 2 time features only: features generated from time axial signals only or it has its own proper function \n",
    "        - if \n",
    "    - 3 frequency features only\n",
    "\n",
    "* magnitude features: features calculated only for magnitude columns\n",
    "    - 1 common features\n",
    "    - 2 time features only\n",
    "    - 3 frequency features only\n",
    "    <br>\n",
    "    <br>\n",
    "* 軸方向の特徴量 : 軸方向の信号 X または Y または Z に対して計算された特徴量。\n",
    "    - 1 共通特徴量 : 時間軸信号と周波数軸信号の両方から同じ方法で生成された特徴量\n",
    "        - 実際には、軸信号から時間および周波数の特徴量を生成するために同じ特徴量関数を使用する。\n",
    "    - 2 時間特徴のみ：時間軸信号のみから生成される特徴、またはそれ自身の適切な関数を持つ特徴  \n",
    "    - 3 周波数特徴量のみ\n",
    "\n",
    "* マグニチュード特徴量：マグニチュードカラムに対してのみ計算される特徴量\n",
    "    - 1 共通特徴量\n",
    "    - 2 時間特徴のみ\n",
    "    - 3 周波数のみの特徴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='step342'></a>\n",
    "### 3.3.1 Common Features\n",
    "- [**Step 1: Define Common Axial Features Functions**](#step3421) \n",
    "- [**Step 2: Define Common Magnitude Features Functions**](#step3422) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3421'></a>\n",
    "**Step 1: Define Common Axial Features Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is dataframe contains 3 columns (3 axial signals X,Y,Z)\n",
    "\n",
    "# mean\n",
    "def mean_axial(df):\n",
    "    array=np.array(df) # convert dataframe into 2D numpy array for efficiency\n",
    "    mean_vector = list(array.mean(axis=0)) # calculate the mean value of each column\n",
    "    return mean_vector # return mean vetor\n",
    "# std\n",
    "def std_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    std_vector = list(array.std(axis=0))# calculate the standard deviation value of each column\n",
    "    return std_vector\n",
    "\n",
    "# mad\n",
    "from statsmodels.robust import mad as median_deviation # import the median deviation function\n",
    "def mad_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    mad_vector = list(median_deviation(array,axis=0)) # calculate the median deviation value of each column\n",
    "    return mad_vector\n",
    "\n",
    "# max\n",
    "\n",
    "def max_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    max_vector=list(array.max(axis=0))# calculate the max value of each column\n",
    "    return max_vector\n",
    "# min\n",
    "def min_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    min_vector=list(array.min(axis=0))# calculate the min value of each column\n",
    "    return min_vector\n",
    "# IQR\n",
    "from scipy.stats import iqr as IQR # import interquartile range function (Q3(column)-Q1(column))\n",
    "def IQR_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    IQR_vector=list(np.apply_along_axis(IQR,0,array))# calculate the inter quartile range value of each column\n",
    "    return IQR_vector\n",
    "\n",
    "\n",
    "# Entropy\n",
    "from scipy.stats import entropy # import the entropy function\n",
    "def entropy_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    entropy_vector=list(np.apply_along_axis(entropy,0,abs(array)))# calculate the entropy value of each column\n",
    "    return entropy_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3422'></a>\n",
    "**Step 2: Define Common Magnitude Features Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mag column : is one column contains one mag signal values\n",
    "# same features mentioned above were calculated for each column\n",
    "\n",
    "# mean\n",
    "def mean_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    mean_value = float(array.mean())\n",
    "    return mean_value\n",
    "\n",
    "# std: standard deviation of mag column\n",
    "def std_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    std_value = float(array.std()) # std value \n",
    "    return std_value\n",
    "\n",
    "# mad: median deviation\n",
    "def mad_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    mad_value = float(median_deviation(array))# median deviation value of mag_column\n",
    "    return mad_value\n",
    "\n",
    "# max\n",
    "def max_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    max_value=float(array.max()) # max value \n",
    "    return max_value\n",
    "# min\n",
    "def min_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    min_value= float(array.min()) # min value\n",
    "    return min_value\n",
    "\n",
    "# IQR\n",
    "def IQR_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    IQR_value=float(IQR(array))# Q3(column)-Q1(column)\n",
    "    return IQR_value\n",
    "\n",
    "# Entropy\n",
    "def entropy_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    entropy_value=float(entropy(array)) # entropy signal\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step343'></a>\n",
    "###  III.3.2. Time Features\n",
    "- [**Step 1: Define Time Axial Features functions**](#step3432)\n",
    "- [**Step 2: Time Axial Features PipeLine**](#step3433)\n",
    "- [**Step 3: Define Time Magnitudes Features functions**](#step3435)\n",
    "- [**Step 4: Time Magnitude Features PipLine**](#step3436)\n",
    "- [**Step 5: Time Features names Generation**](#step3437)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3432'></a>\n",
    "**Step 1: Define Time Axial Features functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to generate time axial features\n",
    "\n",
    "# df is dataframe contains 3 columns (3 axial signals X,Y,Z)\n",
    "# sma\n",
    "def t_sma_axial(df):\n",
    "    array=np.array(df)\n",
    "    sma_axial=float(abs(array).sum())/float(3) # sum of areas under each signal\n",
    "    # → 各軸のarrayの大きさ（つまりx, y, zで３つ）の和を３（軸の数）で割ったもの（つまり平均）\n",
    "    return sma_axial # return sma value\n",
    "\n",
    "# energy\n",
    "def t_energy_axial(df):\n",
    "    array=np.array(df)\n",
    "    energy_vector=list((array**2).sum(axis=0)) # energy value of each df column\n",
    "    # → 各軸のarrayの２乗（各値が２乗）の和がx, y, z軸の３つのリスト\n",
    "    return energy_vector # return energy vector energy_X,energy_Y,energy_Z\n",
    "\n",
    "# AR vector (auto regression coefficients from 1 to 4)\n",
    "\n",
    "# define the arbugr function\n",
    "#auto regression coefficients with using burg method with order from 1 to 4\n",
    "from spectrum import *\n",
    "\n",
    "##############################################################################################\n",
    "# I took this function as it is from this link ------>    https://github.com/faroit/freezefx/blob/master/fastburg.py\n",
    "# This fucntion and the original function arburg in the library spectrum generate the same first 3 coefficients \n",
    "#for all windows the original burg method is low and for some windows it cannot generate all 4th coefficients \n",
    "\n",
    "def _arburg2(X, order):\n",
    "    \"\"\"This version is 10 times faster than arburg, but the output rho is not correct.\n",
    "    returns [1 a0,a1, an-1]\n",
    "    \"\"\"\n",
    "    x = np.array(X)\n",
    "    N = len(x)\n",
    "\n",
    "    if order == 0.:\n",
    "        raise ValueError(\"order must be > 0\")\n",
    "\n",
    "    # Initialisation\n",
    "    # ------ rho, den\n",
    "    rho = sum(abs(x)**2.) / N  # Eq 8.21 [Marple]_\n",
    "    den = rho * 2. * N\n",
    "\n",
    "    # ------ backward and forward errors\n",
    "    ef = np.zeros(N, dtype=complex)\n",
    "    eb = np.zeros(N, dtype=complex)\n",
    "    for j in range(0, N):  # eq 8.11\n",
    "        ef[j] = x[j]\n",
    "        eb[j] = x[j]\n",
    "\n",
    "    # AR order to be stored\n",
    "    a = np.zeros(1, dtype=complex)\n",
    "    a[0] = 1\n",
    "    # ---- rflection coeff to be stored\n",
    "    ref = np.zeros(order, dtype=complex)\n",
    "\n",
    "    E = np.zeros(order+1)\n",
    "    E[0] = rho\n",
    "\n",
    "    for m in range(0, order):\n",
    "        # print m\n",
    "        # Calculate the next order reflection (parcor) coefficient\n",
    "        efp = ef[1:]\n",
    "        ebp = eb[0:-1]\n",
    "        # print efp, ebp\n",
    "        num = -2. * np.dot(ebp.conj().transpose(), efp)\n",
    "        den = np.dot(efp.conj().transpose(),  efp)\n",
    "        den += np.dot(ebp,  ebp.conj().transpose())\n",
    "        ref[m] = num / den\n",
    "\n",
    "        # Update the forward and backward prediction errors\n",
    "        ef = efp + ref[m] * ebp\n",
    "        eb = ebp + ref[m].conj().transpose() * efp\n",
    "\n",
    "        # Update the AR coeff.\n",
    "        a.resize(len(a)+1)\n",
    "        a = a + ref[m] * np.flipud(a).conjugate()\n",
    "\n",
    "        # Update the prediction error\n",
    "        E[m+1] = np.real((1 - ref[m].conj().transpose() * ref[m])) * E[m]\n",
    "        # print 'REF', ref, num, den\n",
    "    return a, E[-1], ref\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "# to generate arburg (order 4) coefficents for 3 columns [X,Y,Z]\n",
    "def t_arburg_axial(df):\n",
    "    # converting signals to 1D numpy arrays for efficiency\n",
    "    array_X=np.array(df[df.columns[0]])\n",
    "    array_Y=np.array(df[df.columns[1]])\n",
    "    array_Z=np.array(df[df.columns[2]])\n",
    "    \n",
    "    AR_X = list(_arburg2(array_X,4)[0][1:].real) # list contains real parts of all 4th coefficients generated from signal_X\n",
    "    AR_Y = list(_arburg2(array_Y,4)[0][1:].real) # list contains real parts of all 4th coefficients generated from signal_Y\n",
    "    AR_Z = list(_arburg2(array_Z,4)[0][1:].real) # list contains real parts of all 4th coefficients generated from signal_Z\n",
    "    \n",
    "    # selecting [AR1 AR2 AR3 AR4] real components for each axis concatenate them in one vector\n",
    "    AR_vector= AR_X + AR_Y+ AR_Z\n",
    "    \n",
    "    \n",
    "    # AR_vector contains 12 values 4values per each axis \n",
    "    return AR_vector\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "def t_corr_axial(df): # it returns 3 correlation features per each 3-axial signals in  time_window\n",
    "    \n",
    "    array=np.array(df)\n",
    "    \n",
    "    Corr_X_Y=float(pearsonr(array[:,0],array[:,1])[0]) # correlation value between signal_X and signal_Y\n",
    "    Corr_X_Z=float(pearsonr(array[:,0],array[:,2])[0]) # correlation value between signal_X and signal_Z\n",
    "    Corr_Y_Z=float(pearsonr(array[:,1],array[:,2])[0]) # correlation value between signal_Y and signal_Z\n",
    "    \n",
    "    corr_vector =[Corr_X_Y, Corr_X_Z, Corr_Y_Z] # put correlation values in list\n",
    "    \n",
    "    return corr_vector \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3433'></a>\n",
    "**Step 2: Time Axial Features PipeLine**\n",
    "<br>\n",
    "<br>\n",
    "時間信号の特徴量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_axial_features_generation(t_window):\n",
    "    \n",
    "    # select axial columns : the first 15 columns\n",
    "    axial_columns=t_window.columns[0:15]\n",
    "    \n",
    "    # select axial columns in a dataframe\n",
    "    axial_df=t_window[axial_columns]\n",
    "    \n",
    "    ## a list will contain all axial features values resulted from applying: \n",
    "    #  common axial features functions and time axial features functions to all time domain signals in t_window\n",
    "    t_axial_features=[]\n",
    "    for col in range(0,15,3):\n",
    "        df=axial_df[axial_columns[col:col+3]] # select each group of 3-axial signal: signal_name[X,Y,Z]\n",
    "        \n",
    "        # apply all common axial features functions and time axial features functions to each 3-axial signals dataframe\n",
    "        mean_vector   = mean_axial(df) # 3values\n",
    "        std_vector    = std_axial(df) # 3 values\n",
    "        mad_vector    = mad_axial(df)# 3 values\n",
    "        max_vector    = max_axial(df)# 3 values\n",
    "        min_vector    = min_axial(df)# 3 values\n",
    "        sma_value     = t_sma_axial(df)# 1 value\n",
    "        energy_vector = t_energy_axial(df)# 3 values\n",
    "        IQR_vector    = IQR_axial(df)# 3 values\n",
    "        entropy_vector= entropy_axial(df)# 3 values\n",
    "        AR_vector     = t_arburg_axial(df)# 3 values\n",
    "        corr_vector   = t_corr_axial(df)# 3 values\n",
    "        \n",
    "        # 40 value per each 3-axial signals\n",
    "        t_3axial_vector= mean_vector + std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + AR_vector + corr_vector\n",
    "        \n",
    "        # append these features to the global list of features\n",
    "        t_axial_features= t_axial_features+ t_3axial_vector\n",
    "    \n",
    "    # t_axial_features contains 200 values = 40 value per each 3axial x 5 tri-axial-signals[X,Y,Z]\n",
    "    return t_axial_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3435'></a>\n",
    "**Step 3: Define Time Magnitudes Features functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to generate time magnitude features\n",
    "\n",
    "\n",
    "# sma: signal magnitude area\n",
    "def t_sma_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    sma_mag=float(abs(array).sum())# signal magnitude area of one mag column\n",
    "    return sma_mag\n",
    "\n",
    "# energy\n",
    "def t_energy_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    energy_value=float((array**2).sum()) # energy of the mag signal\n",
    "    return energy_value\n",
    "\n",
    "\n",
    "\n",
    "# arburg: auto regression coefficients using the burg method\n",
    "def t_arburg_mag(mag_column):\n",
    "    \n",
    "    array = np.array(mag_column)\n",
    "    \n",
    "    AR_vector= list(_arburg2(array,4)[0][1:].real) # AR1, AR2, AR3, AR4 of the mag column\n",
    "    #print(AR_vector)\n",
    "    return AR_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3436'></a>\n",
    "**Step 4: Time Magnitude Features PipLine**\n",
    "<br>\n",
    "<br>\n",
    "時間信号でもマグニチュードの特徴量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_mag_features_generation(t_window):\n",
    "    \n",
    "    # select mag columns : the last 5 columns in a time domain window\n",
    "    \n",
    "    mag_columns=t_window.columns[15:] # mag columns' names\n",
    "    mag_columns=t_window[mag_columns] # mag data frame\n",
    "    \n",
    "    t_mag_features=[] # a global list will contain all time domain magnitude features\n",
    "    \n",
    "    for col in mag_columns: # iterate throw each mag column\n",
    "        \n",
    "        mean_value   = mean_mag(mag_columns[col]) # 1 value\n",
    "        std_value    = std_mag(mag_columns[col])# 1 value\n",
    "        mad_value    = mad_mag(mag_columns[col])# 1 value\n",
    "        max_value    = max_mag(mag_columns[col])# 1 value\n",
    "        min_value    = min_mag(mag_columns[col])# 1 value\n",
    "        sma_value    = t_sma_mag(mag_columns[col])# 1 value\n",
    "        energy_value = t_energy_mag(mag_columns[col])# 1 value\n",
    "        IQR_value    = IQR_mag(mag_columns[col])# 1 value\n",
    "        entropy_value= entropy_mag(mag_columns[col])# 1 value\n",
    "        AR_vector    = t_arburg_mag(mag_columns[col])# 1 value\n",
    "        \n",
    "        # 13 value per each t_mag_column\n",
    "        col_mag_values = [mean_value, std_value, mad_value, max_value, min_value, sma_value, \n",
    "                          energy_value,IQR_value, entropy_value]+ AR_vector\n",
    "        \n",
    "        # col_mag_values will be added to the global list\n",
    "        t_mag_features= t_mag_features+ col_mag_values\n",
    "    \n",
    "    # t_mag_features contains 65 values = 13 values (per each t_mag_column) x 5 (t_mag_columns)\n",
    "    return t_mag_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3437'></a>\n",
    "**Step 5: Time Features names Generation**\n",
    "<br>\n",
    "<br>\n",
    "特徴量名の生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features_names():\n",
    "    # Generating time feature names\n",
    "    \n",
    "    # time domain axial signals' names\n",
    "    t_axis_signals=[['t_body_acc_X','t_body_acc_Y','t_body_acc_Z'],\n",
    "                    ['t_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z'],\n",
    "                    ['t_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z'],    \n",
    "\n",
    "                    ['t_body_gyro_X','t_body_gyro_Y','t_body_gyro_Z'],\n",
    "            ['t_body_gyro_Jerk_X','t_body_gyro_Jerk_Y','t_body_gyro_Jerk_Z'],]\n",
    "    \n",
    "    # time domain magnitude signals' names\n",
    "    magnitude_signals=['t_body_acc_Mag','t_grav_acc_Mag','t_body_acc_jerk_Mag','t_body_gyro_Mag','t_body_gyro_Jerk_Mag']\n",
    "\n",
    "    # functions' names:\n",
    "    t_one_input_features_name1=['_mean()','_std()','_mad()','_max()','_min()']\n",
    "\n",
    "    t_one_input_features_name2=['_energy()','_iqr()','_entropy()']\n",
    "\n",
    "    t_one_input_features_name3=['_AR1()','_AR2()','_AR3()','_AR4()']\n",
    "\n",
    "    correlation_columns=['_Corr(X,Y)','_Corr(X,Z)','_Corr(Y,Z)']\n",
    "\n",
    "    \n",
    "\n",
    "    features=[]# Empty list : it will contain all time domain features' names\n",
    "    \n",
    "    # 特徴量名の決定\n",
    "    for columns in t_axis_signals: # iterate throw  each group of 3-axial signals'\n",
    "        \n",
    "        for feature in t_one_input_features_name1: # iterate throw the first list of functions names\n",
    "            \n",
    "            for column in columns: # iterate throw each axial signal in that group\n",
    "                \n",
    "                newcolumn=column[:-2]+feature+column[-2:] # build the feature name\n",
    "                features.append(newcolumn) # add it to the global list\n",
    "        \n",
    "        sma_column=column[:-2]+'_sma()' # build the feature name sma related to that group\n",
    "        features.append(sma_column) # add the feature to the list\n",
    "        \n",
    "        for feature in t_one_input_features_name2: # same process for the second list of features functions\n",
    "            for column in columns:\n",
    "                newcolumn=column[:-2]+feature+column[-2:]\n",
    "                features.append(newcolumn)\n",
    "        \n",
    "        for column in columns:# same process for the third list of features functions\n",
    "            for feature in t_one_input_features_name3:\n",
    "                newcolumn=column[0:-2]+feature+column[-2:]\n",
    "                features.append(newcolumn)\n",
    "        \n",
    "        for feature in correlation_columns: # adding correlations features\n",
    "            newcolumn=column[0:-2]+feature\n",
    "            features.append(newcolumn)\n",
    "\n",
    "    for columns in magnitude_signals: # iterate throw time domain magnitude column names\n",
    "\n",
    "        # build feature names related to that column\n",
    "        #list 1\n",
    "        for feature in t_one_input_features_name1:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "        # sma feature name\n",
    "        sma_column=columns+'_sma()'\n",
    "        features.append(sma_column)\n",
    "        \n",
    "        # list 2\n",
    "        for feature in t_one_input_features_name2: \n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "        \n",
    "        # list 3\n",
    "        for feature in t_one_input_features_name3:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "    ###########################################################################################################\n",
    "    time_list_features=features\n",
    "    \n",
    "    return time_list_features # return all time domain features' names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step344'></a>\n",
    "### III.3.3. Frequency features\n",
    "- [**Step 1: Define Frequency Axial Features functions**](#step3442)\n",
    "- [**Step 2: Frequency axial features PipeLine**](#step3443)\n",
    "- [**Step 3: Define Frequency Magnitudes Features functions**](#step3445)\n",
    "- [**Step 4: Frequency Magnitude Features PipLine**](#step3446)\n",
    "- [**Step 5: Frequency Features name Generation**](#step3447)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3442'></a>\n",
    "**Step 1: Define Frequency Axial features functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to generate frequency axial features\n",
    "\n",
    "\n",
    "# each df here is dataframe contains 3 columns (3 axial frequency domain signals X,Y,Z)\n",
    "# signals were obtained from frequency domain windows\n",
    "# sma\n",
    "def f_sma_axial(df):\n",
    "    \n",
    "    array=np.array(df)\n",
    "    sma_value=float((abs(array)/math.sqrt(128)).sum())/float(3) # sma value of 3-axial f_signals\n",
    "    \n",
    "    return sma_value\n",
    "\n",
    "# energy\n",
    "def f_energy_axial(df):\n",
    "    \n",
    "    array=np.array(df)\n",
    "    \n",
    "    # spectral energy vector\n",
    "    energy_vector=list((array**2).sum(axis=0)/float(len(array))) # energy of: f_signalX,f_signalY, f_signalZ\n",
    "    \n",
    "    return energy_vector # enrgy veactor=[energy(signal_X),energy(signal_Y),energy(signal_Z)]\n",
    "\n",
    "\n",
    "####### Max Inds and Mean_Freq Functions#######################################\n",
    "# built frequencies list (each column contain 128 value)\n",
    "# duration between each two successive captures is 0.02 s= 1/50hz\n",
    "freqs=sp.fftpack.fftfreq(128, d=0.02) \n",
    "                                \n",
    "\n",
    "# max_Inds\n",
    "def f_max_Inds_axial(df):\n",
    "    array=np.array(df)\n",
    "    max_Inds_X =freqs[array[1:65,0].argmax()+1] # return the frequency related to max value of f_signal X\n",
    "    max_Inds_Y =freqs[array[1:65,1].argmax()+1] # return the frequency related to max value of f_signal Y\n",
    "    max_Inds_Z =freqs[array[1:65,2].argmax()+1] # return the frequency related to max value of f_signal Z\n",
    "    max_Inds_vector= [max_Inds_X,max_Inds_Y,max_Inds_Z]# put those frequencies in a list\n",
    "    return max_Inds_vector\n",
    "\n",
    "# mean freq()\n",
    "def f_mean_Freq_axial(df):\n",
    "    array=np.array(df)\n",
    "    \n",
    "    # sum of( freq_i * f_signal[i])/ sum of signal[i]\n",
    "    mean_freq_X = np.dot(freqs,array[:,0]).sum() / float(array[:,0].sum()) #  frequencies weighted sum using f_signalX\n",
    "    mean_freq_Y = np.dot(freqs,array[:,1]).sum() / float(array[:,1].sum()) #  frequencies weighted sum using f_signalY \n",
    "    mean_freq_Z = np.dot(freqs,array[:,2]).sum() / float(array[:,2].sum()) #  frequencies weighted sum using f_signalZ\n",
    "    \n",
    "    mean_freq_vector=[mean_freq_X,mean_freq_Y,mean_freq_Z] # vector contain mean frequencies[X,Y,Z]\n",
    "    \n",
    "    return  mean_freq_vector\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "########## Skewness & Kurtosis Functions #######################################\n",
    "from scipy.stats import kurtosis       # kurtosis function\n",
    "from scipy.stats import skew           # skewness function\n",
    "    \n",
    "def f_skewness_and_kurtosis_axial(df):\n",
    "    array=np.array(df)\n",
    "    \n",
    "    skew_X= skew(array[:,0])  # skewness value of signal X\n",
    "    kur_X= kurtosis(array[:,0])  # kurtosis value of signal X\n",
    "    \n",
    "    skew_Y= skew(array[:,1]) # skewness value of signal Y\n",
    "    kur_Y= kurtosis(array[:,1])# kurtosis value of signal Y\n",
    "    \n",
    "    skew_Z= skew(array[:,2])# skewness value of signal Z\n",
    "    kur_Z= kurtosis(array[:,2])# kurtosis value of signal Z\n",
    "    \n",
    "    skew_kur_3axial_vector=[skew_X,kur_X,skew_Y,kur_Y,skew_Z,kur_Z] # return the list\n",
    "    \n",
    "    return skew_kur_3axial_vector\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "#################### Bands Energy FUNCTIONS ########################\n",
    "\n",
    "# bands energy levels (start row,end_row) end row not included \n",
    "B1=[(1,9),(9,17),(17,25),(25,33),(33,41),(41,49),(49,57),(57,65)] \n",
    "B2=[(1,17),(17,31),(31,49),(49,65)]\n",
    "B3=[(1,25),(25,49)]\n",
    "\n",
    "def f_one_band_energy(f_signal,band): # f_signal is one column in frequency axial signals in f_window\n",
    "    # band: is one tuple in B1 ,B2 or B3 \n",
    "    f_signal_bounded = f_signal[band[0]:band[1]] # select f_signal components included in the band\n",
    "    energy_value=float((f_signal_bounded**2).sum()/float(len(f_signal_bounded))) # energy value of that band\n",
    "    return energy_value\n",
    "\n",
    "def f_all_bands_energy_axial(df): # df is dataframe contain 3 columns (3-axial f_signals [X,Y,Z])\n",
    "    \n",
    "    E_3_axis =[]\n",
    "    \n",
    "    array=np.array(df)\n",
    "    for i in range(0,3): # iterate throw signals\n",
    "        E1=[ f_one_band_energy( array,( B1 [j][0], B1 [j][1]) ) for j in range(len(B1))] # energy bands1 values of f_signal\n",
    "        E2=[ f_one_band_energy( array,( B2 [j][0], B2 [j][1]) ) for j in range(len(B2))]# energy bands2 values of f_signal\n",
    "        E3=[ f_one_band_energy( array,( B3 [j][0], B3 [j][1]) ) for j in range(len(B3))]# energy bands3 values of f_signal\n",
    "    \n",
    "        E_one_axis = E1+E2+E3 # list of energy bands values of one f_signal\n",
    "        \n",
    "        E_3_axis= E_3_axis + E_one_axis # add values to the global list\n",
    "    \n",
    "    return E_3_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3443'></a>\n",
    "**Step 2: Frequency axial features PipeLine**\\\n",
    "<br>\n",
    "<br>\n",
    "周波数信号の特徴量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_axial_features_generation(f_window):\n",
    "    \n",
    "    \n",
    "    axial_columns=f_window.columns[0:12] # select frequency axial column names\n",
    "    axial_df=f_window[axial_columns] # select frequency axial signals in one dataframe\n",
    "    f_all_axial_features=[] # a global list will contain all frequency axial features values\n",
    "    \n",
    "    \n",
    "    \n",
    "    for col in range(0,12,3):# iterate throw each group of frequency axial signals in a window\n",
    "        \n",
    "        df=axial_df[axial_columns[col:col+3]]  # select each group of 3-axial signals\n",
    "      \n",
    "        # mean\n",
    "        mean_vector                  = mean_axial(df) # 3 values\n",
    "        # std\n",
    "        std_vector                   = std_axial(df) # 3 values\n",
    "        # mad\n",
    "        mad_vector                   = mad_axial(df) # 3 values\n",
    "        # max\n",
    "        max_vector                   = max_axial(df) # 3 values\n",
    "        # min\n",
    "        min_vector                   = min_axial(df) # 3 values\n",
    "        # sma\n",
    "        sma_value                    = f_sma_axial(df)\n",
    "        # energy\n",
    "        energy_vector                = f_energy_axial(df)# 3 values\n",
    "        # IQR\n",
    "        IQR_vector                   = IQR_axial(df) # 3 values\n",
    "        # entropy\n",
    "        entropy_vector               = entropy_axial(df) # 3 values\n",
    "        # max_inds\n",
    "        max_inds_vector              = f_max_Inds_axial(df)# 3 values\n",
    "        # mean_Freq\n",
    "        mean_Freq_vector             = f_mean_Freq_axial(df)# 3 values\n",
    "        # skewness and kurtosis\n",
    "        skewness_and_kurtosis_vector = f_skewness_and_kurtosis_axial(df)# 6 values\n",
    "        # bands energy\n",
    "        bands_energy_vector          = f_all_bands_energy_axial(df) # 42 values\n",
    "\n",
    "        # append all values of each 3-axial signals in a list\n",
    "        f_3axial_features = mean_vector +std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + max_inds_vector + mean_Freq_vector + skewness_and_kurtosis_vector + bands_energy_vector\n",
    "\n",
    "        f_all_axial_features = f_all_axial_features+ f_3axial_features # add features to the global list\n",
    "        \n",
    "    return f_all_axial_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3445'></a>\n",
    "**Step 3: Define Frequency Magnitudes features functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to generate frequency magnitude features\n",
    "\n",
    "# sma\n",
    "def f_sma_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    sma_value=float((abs(array)/math.sqrt(len(mag_column))).sum()) # sma of one mag f_signals\n",
    "    \n",
    "    return sma_value\n",
    "\n",
    "# energy\n",
    "def f_energy_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    # spectral energy value\n",
    "    energy_value=float((array**2).sum()/float(len(array))) # energy value of one mag f_signals\n",
    "    return energy_value\n",
    "\n",
    "\n",
    "####### Max Inds and Mean_Freq Functions#######################################\n",
    "\n",
    "\n",
    "# max_Inds\n",
    "def f_max_Inds_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    \n",
    "    max_Inds_value =float(freqs[array[1:65].argmax()+1]) # freq value related with max component\n",
    "    \n",
    "    return max_Inds_value\n",
    "\n",
    "# mean freq()\n",
    "def f_mean_Freq_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    \n",
    "    mean_freq_value = float(np.dot(freqs,array).sum() / float(array.sum())) # weighted sum of one mag f_signal\n",
    "    \n",
    "    return  mean_freq_value\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "########## Skewness & Kurtosis Functions #######################################\n",
    "\n",
    "from scipy.stats import skew           # skewness\n",
    "def f_skewness_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    skew_value     = float(skew(array)) # skewness value of one mag f_signal\n",
    "    return skew_value\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import kurtosis       # kurtosis\n",
    "def f_kurtosis_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    kurtosis_value = float(kurtosis(array)) # kurotosis value of on mag f_signal\n",
    "\n",
    "    return kurtosis_value\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3446'></a>\n",
    "**Step 4: Define Frequency Magnitude features pipline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_mag_features_generation(f_window):\n",
    "    \n",
    "    # select frequnecy mag columns : the last 4 columns in f_window\n",
    "    mag_columns=f_window.columns[-4:]\n",
    "    mag_columns=f_window[mag_columns]\n",
    "    \n",
    "    f_mag_features=[]\n",
    "    for col in mag_columns: # iterate throw each mag column in f_window\n",
    "        \n",
    "        # calculate common mag features and frequency mag features for each column\n",
    "        mean_value   = mean_mag(mag_columns[col])\n",
    "        std_value    = std_mag(mag_columns[col])\n",
    "        mad_value    = mad_mag(mag_columns[col])\n",
    "        max_value    = max_mag(mag_columns[col])\n",
    "        min_value    = min_mag(mag_columns[col])\n",
    "        sma_value    = f_sma_mag(mag_columns[col])\n",
    "        energy_value = f_energy_mag(mag_columns[col])\n",
    "        IQR_value    = IQR_mag(mag_columns[col])\n",
    "        entropy_value= entropy_mag(mag_columns[col])\n",
    "        max_Inds_value=f_max_Inds_mag(mag_columns[col])\n",
    "        mean_Freq_value= f_mean_Freq_mag (mag_columns[col])\n",
    "        skewness_value=  f_skewness_mag(mag_columns[col])\n",
    "        kurtosis_value = f_kurtosis_mag(mag_columns[col])\n",
    "        # 13 value per each t_mag_column\n",
    "        col_mag_values = [mean_value, std_value, mad_value, max_value, \n",
    "                          min_value, sma_value, energy_value,IQR_value, \n",
    "                          entropy_value, max_Inds_value, mean_Freq_value,\n",
    "                          skewness_value, kurtosis_value ]\n",
    "        \n",
    "        \n",
    "        f_mag_features= f_mag_features+ col_mag_values # append feature values of one mag column to the global list\n",
    "    \n",
    "    # f_mag_features contains 65 values = 13 value (per each t_mag_column) x 4 (f_mag_columns)\n",
    "    return f_mag_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3447'></a>\n",
    "**Step 5: Frequency features name generation**\n",
    "<br>\n",
    "<br>\n",
    "周波数信号の特徴量名生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_features_names():\n",
    "    #Generating Frequency feature names\n",
    "    \n",
    "    # frequency axial signal names \n",
    "    axial_signals=[\n",
    "                    ['f_body_acc_X','f_body_acc_Y','f_body_acc_Z'],\n",
    "                    ['f_body_acc_Jerk_X','f_body_acc_Jerk_Y','f_body_acc_Jerk_Z'],\n",
    "                    ['f_body_gyro_X','f_body_gyro_Y','f_body_gyro_Z'],\n",
    "                    ['f_body_gyro_Jerk_X','f_body_gyro_Jerk_Y','f_body_gyro_Jerk_Z'],\n",
    "                  ]\n",
    "\n",
    "    # frequency magnitude signals\n",
    "    mag_signals=['f_body_acc_Mag','f_body_acc_Jerk_Mag','f_body_gyro_Mag','f_body_gyro_Jerk_Mag']\n",
    "\n",
    "\n",
    "    # features functions names will be applied to f_signals\n",
    "    f_one_input_features_name1=['_mean()','_std()','_mad()','_max()','_min()']\n",
    "\n",
    "    f_one_input_features_name2=['_energy()','_iqr()','_entropy()','_maxInd()','_meanFreq()']\n",
    "\n",
    "    f_one_input_features_name3= ['_skewness()','_kurtosis()']\n",
    "\n",
    "    f_one_input_features_name4=[\n",
    "                                '_BE[1-8]','_BE[9-16]','_BE[17-24]','_BE[25-32]',\n",
    "                                '_BE[33-40]','_BE[41-48]','_BE[49-56]','_BE[57-64]',\n",
    "                                '_BE[1-16]','_BE[17-32]','_BE[33-48]','_BE[49-64]',\n",
    "                                '_BE[1-24]','_BE[25-48]'\n",
    "                               ]\n",
    "    \n",
    "    frequency_features_names=[] # global list of frequency features\n",
    "    \n",
    "    for columns in axial_signals: # iterate throw each group of 3-axial signals\n",
    "        \n",
    "        # iterate throw the first list of features\n",
    "        for feature in f_one_input_features_name1: \n",
    "            for column in columns:# iterate throw each signal name of that group\n",
    "                newcolumn=column[:-2]+feature+column[-2:] # build the full feature name\n",
    "                frequency_features_names.append(newcolumn) # add the feature name to the global list\n",
    "        \n",
    "        # sma feature name\n",
    "        sma_column=column[:-2]+'_sma()'\n",
    "        frequency_features_names.append(sma_column)\n",
    "\n",
    "        # iterate throw the first list of features\n",
    "        for feature in f_one_input_features_name2:\n",
    "            for column in columns:\n",
    "                newcolumn=column[:-2]+feature+column[-2:]\n",
    "                frequency_features_names.append(newcolumn)\n",
    "        \n",
    "        # iterate throw each signal name of that group\n",
    "        for column in columns:\n",
    "            for feature in f_one_input_features_name3: # iterate throw [skewness ,kurtosis]\n",
    "                newcolumn=column[:-2]+feature+column[-2:] # build full feature name\n",
    "                frequency_features_names.append(newcolumn) # append full feature names\n",
    "        \n",
    "        # same process above will be applied to list number 4\n",
    "        for column in columns:\n",
    "            for feature in f_one_input_features_name4:\n",
    "                newcolumn=column[:-2]+feature+column[-2:]\n",
    "                frequency_features_names.append(newcolumn)\n",
    "   \n",
    "    #################################################################################################################\n",
    "    # generate frequency mag features names\n",
    "    for column in mag_signals:# iterate throw each frequency mag signal name\n",
    "        for feature in f_one_input_features_name1:# iterate throw the first list of features functions names\n",
    "            frequency_features_names.append(column+feature) # build the full feature name and add it to the global list\n",
    "\n",
    "        sma_column=column+'_sma()' # build the sma full feature name\n",
    "        frequency_features_names.append(sma_column) # add it to the global list\n",
    "\n",
    "        for feature in f_one_input_features_name2:# iterate throw the second list of features functions names\n",
    "            frequency_features_names.append(column+feature)# build the full feature name and add it to the global list\n",
    "        \n",
    "        for feature in f_one_input_features_name3:# iterate throw the third list of features functions names\n",
    "            frequency_features_names.append(column+feature)# build the full feature name and add it to the global list\n",
    "    ####################################################################################################################\n",
    "    \n",
    "    return frequency_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step345'></a>\n",
    "### III.3.4. Additional features（その他の特徴量）\n",
    "- [**Step 1: Define Addtional features functions**](#step3451)\n",
    "- [**Step 2: Define Additional features names**](#step3452)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3452'></a>\n",
    "**Step 1: Define Addtional features functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Angles Functions ####################################\n",
    "from math import acos # inverse of cosinus function\n",
    "from math import sqrt # square root function\n",
    "\n",
    "########Euclidian magnitude 3D############\n",
    "def magnitude_vector(vector3D): # vector[X,Y,Z]\n",
    "    return sqrt((vector3D**2).sum()) # eulidian norm of that vector\n",
    "\n",
    "###########angle between two vectors in radian ###############\n",
    "def angle(vector1, vector2):\n",
    "    vector1_mag=magnitude_vector(vector1) # euclidian norm of V1\n",
    "    vector2_mag=magnitude_vector(vector2) # euclidian norm of V2\n",
    "   \n",
    "    scalar_product=np.dot(vector1,vector2) # scalar product of vector 1 and Vector 2\n",
    "    cos_angle=scalar_product/float(vector1_mag*vector2_mag) # the cosinus value of the angle between V1 and V2\n",
    "    \n",
    "    # just in case some values were added automatically\n",
    "    if cos_angle>1:\n",
    "        cos_angle=1\n",
    "    elif cos_angle<-1:\n",
    "        cos_angle=-1\n",
    "    \n",
    "    angle_value=float(acos(cos_angle)) # the angle value in radian\n",
    "    return angle_value # in radian.\n",
    "\n",
    "################## angle_features ############################\n",
    "def angle_features(t_window): # it returns 7 angles per window\n",
    "    angles_list=[]# global list of angles values\n",
    "    \n",
    "    # mean value of each column t_body_acc[X,Y,Z]\n",
    "    V2_columns=['t_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z']\n",
    "    V2_Vector=np.array(t_window[V2_columns].mean()) # mean values\n",
    "    \n",
    "    # angle 0: angle between (t_body_acc[X.mean,Y.mean,Z.mean], t_gravity[X.mean,Y.mean,Z.mean])\n",
    "    V1_columns=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z']\n",
    "    V1_Vector=np.array(t_window[V1_columns].mean()) # mean values of t_body_acc[X,Y,Z]\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector)) # angle between the vectors added to the global list\n",
    "    \n",
    "    # same process is applied to ither signals\n",
    "    # angle 1: (t_body_acc_jerk[X.mean,Y.mean,Z.mean],t_gravity[X.mean,Y.mean,Z.mean]\n",
    "    V1_columns=['t_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z']\n",
    "    V1_Vector=np.array(t_window[V1_columns].mean())\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    # angle 2: (t_body_gyro[X.mean,Y.mean,Z.mean],t_gravity[X.mean,Y.mean,Z.mean]\n",
    "    V1_columns=['t_body_gyro_X','t_body_gyro_Y','t_body_gyro_Z']\n",
    "    V1_Vector=np.array(t_window[V1_columns].mean())\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    # angle 3: (t_body_gyro_jerk[X.mean,Y.mean,Z.mean],t_gravity[X.mean,Y.mean,Z.mean]\n",
    "    V1_columns=['t_body_gyro_jerk_X','t_body_gyro_jerk_Y','t_body_gyro_jerk_Z']\n",
    "    V1_Vector=np.array(t_window[V1_columns].mean())\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    #################################################################################\n",
    "    \n",
    "    # V1 vector in this case is the X axis itself [1,0,0]\n",
    "    # angle 4: ([X_axis],t_gravity[X.mean,Y.mean,Z.mean])   \n",
    "    V1_Vector=np.array([1,0,0])\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    # V1 vector in this case is the Y axis itself [0,1,0]\n",
    "    # angle 5: ([Y_acc_axis],t_gravity[X.mean,Y.mean,Z.mean]) \n",
    "    V1_Vector=np.array([0,1,0])\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    # V1 vector in this case is the Z axis itself [0,0,1]\n",
    "    # angle 6: ([Z_acc_axis],t_gravity[X.mean,Y.mean,Z.mean])\n",
    "    V1_Vector=np.array([0,0,1])\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    return angles_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3453'></a>\n",
    "**Step 2: Define Additional features names**\n",
    "<br>\n",
    "<br>\n",
    "加える，特徴量の名前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_columns=['angle0()','angle1()','angle2()','angle3()','angle4()','angle5()','angle6()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step35'></a>\n",
    "## IV. Generating Final Datasets\t\t\n",
    "- [**Step 1: Dataset Generation PipeLine**](#step352)\n",
    "- [**Step 2: Generating Dataset type I**](#step353)\n",
    "- [**Step 3: Generating Dataset type II**](#step354)\n",
    "- [**Discussion**](#step355)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step352'></a>\n",
    "#### Step 1 : Define Datasets generation PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conctenate all features names lists and we add two other columns activity ids and user ids will be related to each row\n",
    "# 全特徴量\n",
    "all_columns=time_features_names()+frequency_features_names()+angle_columns+['activity_Id','user_Id']\n",
    "\n",
    "def Dataset_Generation_PipeLine(t_dic,f_dic):\n",
    "    # t_dic is a dic contains time domain windows\n",
    "    # f_dic is a dic contains frequency domain windows\n",
    "    # f_dic should be the result of applying fft to t_dic\n",
    "    \n",
    "    final_Dataset=pd.DataFrame(data=[],columns= all_columns) # build an empty dataframe to append rows\n",
    "    \n",
    "    t_keys = sorted(t_dic.keys())\n",
    "    f_keys = sorted(f_dic.keys())\n",
    "    \n",
    "    for i in range(len(t_dic)): # iterate throw each window\n",
    "\n",
    "        # t_window and f_window should have the same window id included in their keys\n",
    "        t_key = t_keys[i] # extract the key of t_window\n",
    "        f_key = f_keys[i] # extract the key of f_window \n",
    "        \n",
    "        t_window = t_dic[t_key] # extract the t_window\n",
    "        f_window = f_dic[f_key] # extract the f_window\n",
    "\n",
    "        window_user_id = int(t_key[-8:-6]) # extract the user id from window's key\n",
    "        window_activity_id = int(t_key[-2:]) # extract the activity id from the windows key\n",
    "\n",
    "        # generate all time features from t_window \n",
    "        time_features = t_axial_features_generation(t_window) + t_mag_features_generation(t_window)\n",
    "        \n",
    "        # generate all frequency features from f_window\n",
    "        frequency_features = f_axial_features_generation(f_window) + f_mag_features_generation(f_window)\n",
    "        \n",
    "        # Generate addtional features from t_window\n",
    "        additional_features= angle_features(t_window)\n",
    "        \n",
    "        # concatenate all features and append the activity id and the user id\n",
    "        row= time_features + frequency_features + additional_features + [int(window_activity_id), int(window_user_id)]\n",
    "        \n",
    "        # go to the first free index in the dataframe\n",
    "        # dfの末尾indexを取得\n",
    "        free_index=len(final_Dataset)\n",
    "        \n",
    "        # append the row\n",
    "        final_Dataset.loc[free_index]= row\n",
    "        \n",
    "    return final_Dataset # return the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step353'></a>\n",
    "#### Step 2: Generating Dataset type I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply datasets generation pipeline to time and frequency windows type I\n",
    "Dataset_type_I= Dataset_Generation_PipeLine(t_dic_win_type_I,f_dic_win_type_I)\n",
    "\n",
    "print('The shape of Dataset type I is :',Dataset_type_I.shape) # shape of the dataset type I\n",
    "display(Dataset_type_I.describe()) # statistical description\n",
    "display(Dataset_type_I.head(3)) # the first three rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step354'></a>\n",
    "#### Step 3: Generating Dataset type II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply datasets generation pipeline to time and frequency windows type II\n",
    "Dataset_type_II=Dataset_Generation_PipeLine(t_dic_win_type_II,f_dic_win_type_II)\n",
    "print('The shape of Dataset type II is :',Dataset_type_II.shape)\n",
    "display(Dataset_type_II.describe())\n",
    "display(Dataset_type_II.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "# V. Datasets Exportation\n",
    "- [**Export both Datasets**](#step42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step42'></a>\n",
    "**Export both Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide datasets type I and II in two parts to reduce the size for github uploading\n",
    "Dataset_type_I_part1=Dataset_type_I.iloc[0:5001]\n",
    "Dataset_type_I_part2=Dataset_type_I.iloc[5001:]\n",
    "\n",
    "Dataset_type_II_part1=Dataset_type_II.iloc[0:6001]\n",
    "Dataset_type_II_part2=Dataset_type_II.iloc[6001:]\n",
    "\n",
    "# Define paths and files' names\n",
    "path1=\"feature_data_HAPT/Dataset_I_part1.csv\"\n",
    "path2=\"feature_data_HAPT/Dataset_I_part2.csv\"\n",
    "path3=\"feature_data_HAPT/Dataset_II_part1.csv\"   \n",
    "path4=\"feature_data_HAPT/Dataset_II_part2.csv\"   \n",
    "\n",
    "# Export all part into a CSV form in : \"feature_data_HAPT/\"\n",
    "Dataset_type_I_part1.to_csv(path_or_buf=path1, na_rep='NaN',  \n",
    "             columns=None, header=True, \n",
    "             index=False, mode='w', \n",
    "             encoding='utf-8',  \n",
    "             line_terminator='\\n', \n",
    "             )\n",
    "Dataset_type_I_part2.to_csv(path_or_buf=path2, na_rep='NaN',  \n",
    "             columns=None, header=True, \n",
    "             index=False, mode='w', \n",
    "             encoding='utf-8',  \n",
    "             line_terminator='\\n', \n",
    "             )\n",
    "\n",
    "Dataset_type_II_part1.to_csv(path_or_buf=path3, na_rep='NaN',  \n",
    "             columns=None, header=True, \n",
    "             index=False, mode='w', \n",
    "             encoding='utf-8',  \n",
    "             line_terminator='\\n', \n",
    "             )\n",
    "Dataset_type_II_part2.to_csv(path_or_buf=path4, na_rep='NaN',  \n",
    "             columns=None, header=True, \n",
    "             index=False, mode='w', \n",
    "             encoding='utf-8',  \n",
    "             line_terminator='\\n', \n",
    "             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
