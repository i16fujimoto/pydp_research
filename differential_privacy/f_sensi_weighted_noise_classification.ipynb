{"cells":[{"cell_type":"markdown","metadata":{"id":"d9GnJBMbztj8"},"source":["# Applying Machine Learning Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDF4vw-fztj-"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import itertools\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from datetime import datetime\n","\n","from sklearn import linear_model\n","# metrics: 計測 → 評価\n","from sklearn import metrics\n","\n","from sklearn.model_selection import GridSearchCV\n","# カーネルなしSVM（SVC: Support Vector Classifierの略）\n","from sklearn.svm import LinearSVC\n","# カーネル法を用いたSVM\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1504,"status":"ok","timestamp":1666465063844,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"z_zqrUgZZUYB","outputId":"c8fdd302-24b4-4e13-bd92-bd2a7b6453fd"},"outputs":[],"source":["data1 = pd.read_csv('Data_UCI_not_noise_1.csv')\n","data2 = pd.read_csv('Data_UCI_not_noise_2.csv')\n","\n","df_concat = pd.concat([data1, data2])\n","\n","print(df_concat.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666465063844,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"jK4L2ogo6zXA","outputId":"2d531e5f-6de5-4024-d49c-0310e31803c3"},"outputs":[],"source":["features_act = list()\n","features_uid = list()\n","\n","with open('act_feature_importance.txt', 'r') as f:\n","    features_act = f.read().splitlines()\n","\n","with open('uid_feature_importance.txt', 'r') as f:\n","    features_uid = f.read().splitlines()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cbAim_GcDDN"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","def split_train_test_data(df, id):\n","  X = df.drop(['user_Id', 'activity_Id'], axis=1)\n","  y = df[id]\n","\n","  return train_test_split(X, y, test_size=0.33, random_state=42) # X_train, X_test, y_train, y_test\n","\n","  # for i in X_train.columns:\n","  #   print(X_train[i].dtype)\n","  # print(X_train.dtypes)\n","\n","  # # 訓練用\n","  # X_train = train.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n","  # y_train = train['subject']\n","\n","  # # テスト用\n","  # X_test = test.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n","  # y_test = test['subject']\n","\n","  # print('X_train and y_train : ({},{})'.format(X_train.shape, y_train.shape))\n","  # print('X_test  and y_test  : ({},{})'.format(X_test.shape, y_test.shape))"]},{"cell_type":"markdown","metadata":{"id":"1dWE12XiztkB"},"source":["Let's make a function to plot the confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QTV20ztztkB"},"outputs":[],"source":["plt.rcParams['font.family'] = 'DejaVu Sans'\n","\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n","    \n","    # normalize → 正規化\n","    if normalize:\n","        # cmの値は予測結果と実際の値の一致数なので，それを列の合計で割ると確率\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # np.newaxisは次元を追加\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=90)\n","    plt.yticks(tick_marks, classes)\n","    \n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        # plt.text(): 座標（x, y），表示するテキスト，文字位置，色指定\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment='center', color='white' if cm[i, j] > thresh else 'black')\n","    \n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"markdown","metadata":{"id":"Qtj-5lXyztkB"},"source":["Let's make a function to run any model specified"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMHjny0sztkC"},"outputs":[],"source":["# 任意のモデルを実行\n","def perform_model_epsilon(model, X_train, y_train, X_test, y_test, class_labels, cm_nomalize=True, print_cm=True, cm_cmap=plt.cm.Greens):\n","    \n","    # to store results at various phases\n","    results = dict()\n","    \n","    # time at which model starts training \n","    train_start_time = datetime.now()\n","    print('training the model...')\n","    model.fit(X_train, y_train)\n","    print('Done')\n","    train_end_time = datetime.now()\n","    results['training_time'] = train_end_time - train_start_time\n","    print('==> training time:- {}\\n'.format(results['training_time']))\n","    \n","    # predict test data\n","    print('Predicting test data')\n","    test_start_time = datetime.now()\n","    y_pred = model.predict(X_test)\n","    test_end_time = datetime.now()\n","    results['testing_time'] = test_end_time - test_start_time\n","    print('==> testing time:- {}\\n'.format(results['testing_time']))\n","    # 予測結果を格納\n","    results['predicted'] = y_pred\n","    \n","    # calculate overall accuracy of the model\n","    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n","    # store accuracy in results\n","    results['accuracy'] = accuracy\n","    print('==> Accuracy:- {}\\n'.format(accuracy))\n","    \n","    # confusion matrix\n","    cm = metrics.confusion_matrix(y_test, y_pred)\n","    results['confusion_matrix'] = cm\n","    # output confusion matrix\n","    if print_cm:\n","        print('\\n ********Confusion Matrix********')\n","        print('\\n {}'.format(cm))\n","    \n","    # plot confusion matrix\n","    plt.figure(figsize=(15, 15))\n","    plt.grid(b=False) # グリッドを非表示\n","    plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized Confusion Matrix', cmap = cm_cmap)\n","    plt.show()\n","    \n","    # get classification report\n","    # print('****************| Classifiction Report |****************')\n","    # classification_report = metrics.classification_report(y_test, y_pred)\n","    \n","    # # store report in results\n","    # results['classification_report'] = classification_report\n","    # print(classification_report)\n","    \n","    # get f1 score\n","    f1 = metrics.f1_score(y_test, y_pred, average=\"macro\")\n","    print(\"\\n F1 Score:{}\".format(f1))\n","    \n","    # add the trained model to the results\n","    results['model'] = model\n","    \n","    return results, f1\n","    "]},{"cell_type":"markdown","metadata":{"id":"rSSBk5dyztkC"},"source":["Make function to print the gridsearch Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXXnVumOztkD"},"outputs":[],"source":["# 同一モデルでの値を表示\n","def print_grid_search_attributes(model):\n","    \n","    # Estimator that gave highest score among all the estimators formed in GridSearch\n","    print('\\n\\n==> Best Estimator:')\n","    print('\\t{}\\n'.format(model.best_estimator_))\n","    \n","    # parameters that gave best results while perfoming grid search\n","    print('\\n==> Best parameters:')\n","    print('\\tParameters of best estimator : {}'.format(model.best_params_))\n","    \n","    # number of cross validation splits\n","    print('\\n==> No. of CrossValidation sets:')\n","    print('\\tTotal nmber of cross validation sets: {}'.format(model.n_splits_))\n","    \n","    # Average cross validated score of the best estimator, from the Grid Search\n","    print('\\n==> Best Score:')\n","    print('\\tAverage Cross Validate scores of Best estimator : {}'.format(model.best_score_))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qI03h0T56DhI"},"outputs":[],"source":["labels_act = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n","\n","labels_uid = list()\n","for i in range(1, 31):\n","    labels_uid.append(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":274,"status":"ok","timestamp":1666514090597,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"Nig36Y0TjK5J","outputId":"28585d4e-c928-4565-f01c-7ad240c6c52e"},"outputs":[],"source":["n = 119\n","# f_add_weighted_noise = set()\n","# for f in features_uid[:n]:\n","#   f_add_weighted_noise.add(f)\n","# print(f_add_weighted_noise)\n","\n","# n = 211\n","f_add_weighted_noise = set()\n","for f in features_uid[:n]:\n","  # 重複していないものを入れる\n","  if f not in features_act[:n]:\n","    f_add_weighted_noise.add(f)\n","print(len(f_add_weighted_noise))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2N0j4lrhmsko"},"outputs":[],"source":["weighted_epsilon_array = [0.001, 0.005, 0.008, 0.01, 0.015, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1]\n","weighted_epsilon = weighted_epsilon_array[0]"]},{"cell_type":"markdown","metadata":{"id":"8HPtUoULMh10"},"source":["## 2. Random Forest Classifier"]},{"cell_type":"markdown","metadata":{"id":"MTdmwcPkOFZ1"},"source":["## HAR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4769543,"status":"ok","timestamp":1666522566593,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"cTuqefPHMk0-","outputId":"a039742a-5426-4253-d4dd-b2281321ba82"},"outputs":[],"source":["import warnings\n","# 収束しなかった場合のwarning\n","from sklearn.exceptions import ConvergenceWarning\n","# warningを無視する\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n","\n","epsilons = [1.1, 1.2, 1.3, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.2]\n","acc_array_random_har = list()\n","f1_array_random_har = list()\n","\n","for epsilon in epsilons:\n","\n","  print('--------')\n","  print('epsilon:  '+ str(epsilon))\n","  print('--------')\n","\n","  X_train, X_test, y_train, y_test = split_train_test_data(df_concat, 'activity_Id')\n","  X_train.max(axis=0)\n","  X_train.min(axis=0)\n","\n","  sensitivity_df = X_train.max(axis=0) - X_train.min(axis=0)\n","\n","  # for feature in f_add_weighted_noise:\n","  #   print(X_train[feature])\n","  #   X_train[feature] = X_train[feature].apply(lambda x: x + np.random.laplace(0, 1.0/weighted_epsilon))\n","  #   X_test[feature] = X_test[feature].apply(lambda x: x + np.random.laplace(0, 1.0/weighted_epsilon))\n","  #   print(X_train[feature])\n","\n","  for col in X_train:\n","      # default sensitivity = 1.0\n","    if col not in f_add_weighted_noise:\n","      # print(\"------epsilon--------\")\n","      \n","      # Laplace\n","      # X_train[col] = X_train[col].apply(lambda x: x + np.random.laplace(0, 1.0/epsilon))\n","      # X_test[col] = X_test[col].apply(lambda x: x + np.random.laplace(0, 1.0/epsilon))\n","      \n","      # # Gausian\n","      X_train[col] = X_train[col].apply(lambda x: x + np.random.normal(0, 1.0/epsilon))\n","      X_test[col] = X_test[col].apply(lambda x: x + np.random.normal(0, 1.0/epsilon))\n","    else :\n","      # print(\"------weighted_epsilon--------\")\n","      X_train[col] = X_train[col].apply(lambda x: x + np.random.laplace(0, sensitivity_df[col]/weighted_epsilon))\n","      X_test[col] = X_test[col].apply(lambda x: x + np.random.laplace(0, sensitivity_df[col]/weighted_epsilon))\n","\n","    \n","  params = {'n_estimators': np.arange(150,201,10), 'max_depth':np.arange(6,15,2)}\n","  rfc = RandomForestClassifier()\n","  rfc_grid = GridSearchCV(rfc, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n","  rfc_grid_results, f1 = perform_model_epsilon(rfc_grid, X_train.values, y_train.values, X_test.values, y_test.values, class_labels=labels_act)\n","  f1_array_random_har.append(f1)\n","  acc_array_random_har.append(rfc_grid_results['accuracy'])\n","  \n","  # observe the attributes of the model \n","  print_grid_search_attributes(rfc_grid_results['model'])\n","    \n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","ax.plot(epsilons[::-1], f1_array_random_har[::-1], label='Random Forest in Noise')\n","ax.set_xlabel('epsilon')\n","ax.set_ylabel('accuracy')\n","plt.legend(loc='best')\n","plt.show()\n","\n","print(f1_array_random_har)"]},{"cell_type":"markdown","metadata":{"id":"bTETiJB5OCs6"},"source":["## UID"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3673831,"status":"ok","timestamp":1666517797060,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"icS8VaayOBN2","outputId":"21575c0e-dd5e-4df0-b16f-43082861f261"},"outputs":[],"source":["import warnings\n","# 収束しなかった場合のwarning\n","from sklearn.exceptions import ConvergenceWarning\n","# warningを無視する\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n","\n","# epsilons = [0.01, 0.05, 0.1, 0.3, 0.5, 1.0, 1.5, 2.0, 2.5, 3, 4, 5]\n","acc_array_random_uid = list()\n","f1_array_random_uid = list()\n","\n","\n","epsilons = [1.1, 1.2, 1.3, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.2]\n","\n","for epsilon in epsilons:\n","\n","  print('--------')\n","  print('epsilon:  '+ str(epsilon))\n","  print('--------')\n","\n","  X_train, X_test, y_train, y_test = split_train_test_data(df_concat, 'user_Id')\n","  X_train.max(axis=0)\n","  X_train.min(axis=0)\n","\n","  sensitivity_df = X_train.max(axis=0) - X_train.min(axis=0)\n","\n","  # for feature in f_add_weighted_noise:\n","  #   print(X_train[feature])\n","  #   X_train[feature] = X_train[feature].apply(lambda x: x + np.random.laplace(0, 1.0/weighted_epsilon))\n","  #   X_test[feature] = X_test[feature].apply(lambda x: x + np.random.laplace(0, 1.0/weighted_epsilon))\n","  #   print(X_train[feature])\n","\n","  for col in X_train:\n","      # default sensitivity = 1.0\n","    if col not in f_add_weighted_noise:\n","      # print(\"------epsilon--------\")\n","      \n","      # Laplace\n","      # X_train[col] = X_train[col].apply(lambda x: x + np.random.laplace(0, 1.0/epsilon))\n","      # X_test[col] = X_test[col].apply(lambda x: x + np.random.laplace(0, 1.0/epsilon))\n","      \n","      # Gausian\n","      X_train[col] = X_train[col].apply(lambda x: x + np.random.normal(0, 1.0/epsilon))\n","      X_test[col] = X_test[col].apply(lambda x: x + np.random.normal(0, 1.0/epsilon))\n","    else :\n","      # print(\"------weighted_epsilon--------\")\n","      X_train[col] = X_train[col].apply(lambda x: x + np.random.laplace(0, sensitivity_df[col]/weighted_epsilon))\n","      X_test[col] = X_test[col].apply(lambda x: x + np.random.laplace(0, sensitivity_df[col]/weighted_epsilon))\n","\n","  \n","  params = {'n_estimators': np.arange(150,171,10), 'max_depth':np.arange(6,15,2)}\n","  rfc = RandomForestClassifier()\n","  rfc_grid = GridSearchCV(rfc, param_grid=params, cv=5, verbose=1, n_jobs=-1)\n","  rfc_grid_results, f1 = perform_model_epsilon(rfc_grid,  X_train.values, y_train.values, X_test.values, y_test.values, class_labels=labels_uid)\n","  f1_array_random_uid.append(f1)\n","  acc_array_random_uid.append(rfc_grid_results['accuracy'])\n","  \n","  # observe the attributes of the model \n","  print_grid_search_attributes(rfc_grid_results['model'])\n","    \n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","ax.plot(epsilons[::-1], f1_array_random_uid[::-1], label='Random Forest in Noise')\n","ax.set_xlabel('epsilon')\n","ax.set_ylabel('accuracy')\n","plt.legend(loc='best')\n","plt.show()\n","\n","print(f1_array_random_uid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666522566593,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"fy9TbVQuuO1M","outputId":"8c755ece-a66b-4ff0-ffe0-045383fd7cd6"},"outputs":[],"source":["# epsilons = [1.1, 1.2, 1.3, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.2]\n","print(f1_array_random_har)\n","print(f1_array_random_uid)\n","print(acc_array_random_har)\n","print(acc_array_random_uid)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666522566594,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"4akcUwO_DqPL","outputId":"3181bbd0-e75a-41f2-fea4-0523efb43b73"},"outputs":[],"source":["print(weighted_epsilon)\n","print(n)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666527574947,"user":{"displayName":"Ryusei Fujimoto","userId":"00131692607297579166"},"user_tz":-540},"id":"kG18DrB9yJtU","outputId":"a1685b3a-4169-4ebc-e05d-c914e258e651"},"outputs":[],"source":["print(epsilons)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIE3NQ9VztkH"},"outputs":[],"source":["def beep():\n","  from google.colab import output\n","  output.eval_js('new Audio(\\\n","\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\")\\\n",".play()') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWvCZ6tBdmDm"},"outputs":[],"source":["beep()"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"interpreter":{"hash":"9cf94c4db506e4a0ca4ebab029940422049a9be7b0bda48bd1038b6fecafe843"},"kernelspec":{"display_name":"Python 3.9.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
